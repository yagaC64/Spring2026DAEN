{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed030305",
   "metadata": {},
   "source": [
    "# Puerto Rico Census Risk Features (Town, ZIP, Coordinates)\n",
    "\n",
    "This notebook pulls the latest available ACS 5-year Census data for Puerto Rico, builds risk features, and exports model-ready tables for municipios, ZIP Code Tabulation Areas (ZCTAs), and town coordinate points.\n",
    "\n",
    "GitHub: https://github.com/yagaC64/Spring2026DAEN\n",
    "\n",
    "License: https://github.com/yagaC64/Spring2026DAEN/blob/main/LICENSE\n",
    "\n",
    "Data sources in this notebook are public/open:\n",
    "- U.S. Census API (ACS 5-year)\n",
    "- U.S. Census Geocoder API (optional point-to-geography lookup)\n",
    "- Local PR town coordinate lookup file (`Puerto_RIco_Towns_Coords.xlsx`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef6ce0",
   "metadata": {},
   "source": [
    "## Run Instructions\n",
    "\n",
    "1. Run cells top-to-bottom.\n",
    "2. Optional `.env` values:\n",
    "   - `CENSUS_API_KEY=<your_key>` (optional; useful above 500 requests/day/IP)\n",
    "   - `ACS_YEAR=2024` (optional; if omitted, notebook auto-detects latest available year)\n",
    "   - `PR_TOWNS_COORDS_FILE=JupyterNotebooks/Puerto_RIco_Towns_Coords.xlsx` (optional override)\n",
    "   - `ENABLE_CENSUS_GEOCODER=1` (optional; enriches town points with county/tract GEOIDs)\n",
    "3. Outputs are written to `JupyterNotebooks/outputs/census_pr/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install and import dependencies\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "from datetime import datetime, UTC\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "required_packages = [\"pandas\", \"requests\", \"openpyxl\", \"python-dotenv\", \"numpy\"]\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", *required_packages])\n",
    "print(\"Installation complete.\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except ImportError:\n",
    "    display = print\n",
    "\n",
    "# Load .env if present in repo root or current directory\n",
    "load_dotenv(Path.cwd() / \".env\")\n",
    "load_dotenv(Path.cwd().parent / \".env\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(\"census-pr-risk\")\n",
    "\n",
    "print(\"Cell 1 complete: dependencies installed and imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74050119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration and helper functions\n",
    "PR_STATE_FIPS = \"72\"\n",
    "ACS_DATASET = \"acs/acs5\"\n",
    "DEFAULT_TOWNS_FILE = \"Puerto_RIco_Towns_Coords.xlsx\"\n",
    "OUTPUT_DIR = Path(\"JupyterNotebooks/outputs/census_pr\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CENSUS_API_KEY = os.environ.get(\"CENSUS_API_KEY\")\n",
    "ACS_YEAR_ENV = os.environ.get(\"ACS_YEAR\", \"\").strip()\n",
    "ENABLE_CENSUS_GEOCODER = os.environ.get(\"ENABLE_CENSUS_GEOCODER\", \"\").lower() in {\"1\", \"true\", \"yes\"}\n",
    "\n",
    "ACS_VARIABLES = {\n",
    "    \"population\": \"B01003_001E\",\n",
    "    \"median_income\": \"B19013_001E\",\n",
    "    \"poverty_universe\": \"B17001_001E\",\n",
    "    \"poverty_count\": \"B17001_002E\",\n",
    "    \"housing_units\": \"B25001_001E\",\n",
    "    \"occupied_units\": \"B25002_002E\",\n",
    "    \"vacant_units\": \"B25002_003E\",\n",
    "    \"no_vehicle_owner\": \"B25044_003E\",\n",
    "    \"no_vehicle_renter\": \"B25044_010E\",\n",
    "}\n",
    "\n",
    "CENSUS_SENTINEL_MISSING = {\n",
    "    -666666666, -555555555, -333333333, -222222222, -111111111, -999999999\n",
    "}\n",
    "\n",
    "\n",
    "def resolve_file(filename, env_var=None, search_roots=None):\n",
    "    if env_var:\n",
    "        env_value = os.environ.get(env_var)\n",
    "        if env_value:\n",
    "            candidate = Path(env_value).expanduser()\n",
    "            if candidate.exists():\n",
    "                return candidate\n",
    "            raise FileNotFoundError(f\"{env_var} is set but file was not found: {candidate}\")\n",
    "\n",
    "    roots = search_roots or [Path.cwd(), Path.cwd() / \"JupyterNotebooks\", Path.cwd().parent]\n",
    "    for root in roots:\n",
    "        if not root.exists():\n",
    "            continue\n",
    "        found = next(root.rglob(filename), None)\n",
    "        if found:\n",
    "            return found\n",
    "    raise FileNotFoundError(f\"Could not find {filename}. Set {env_var} or place the file under this repo.\")\n",
    "\n",
    "\n",
    "def normalize_text(value):\n",
    "    if pd.isna(value):\n",
    "        return \"\"\n",
    "    text = str(value).strip()\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = \"\".join(ch for ch in text if not unicodedata.combining(ch))\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "\n",
    "def parse_zip_codes(value):\n",
    "    if pd.isna(value):\n",
    "        return []\n",
    "    return sorted(set(re.findall(r\"\\b\\d{5}\\b\", str(value))))\n",
    "\n",
    "\n",
    "def to_numeric(df, columns):\n",
    "    for col in columns:\n",
    "        numeric_series = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        df[col] = numeric_series.replace(list(CENSUS_SENTINEL_MISSING), np.nan)\n",
    "    return df\n",
    "\n",
    "\n",
    "def safe_divide(numerator, denominator):\n",
    "    result = numerator / denominator.replace({0: np.nan})\n",
    "    return result.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "def minmax_score(series, invert=False):\n",
    "    values = pd.to_numeric(series, errors=\"coerce\")\n",
    "    vmin = values.min(skipna=True)\n",
    "    vmax = values.max(skipna=True)\n",
    "    if pd.isna(vmin) or pd.isna(vmax) or vmin == vmax:\n",
    "        score = pd.Series(np.nan, index=values.index, dtype=\"float64\")\n",
    "    else:\n",
    "        score = (values - vmin) / (vmax - vmin)\n",
    "    if invert:\n",
    "        score = 1 - score\n",
    "    return score.clip(lower=0, upper=1)\n",
    "\n",
    "\n",
    "def find_latest_acs5_year(state_fips=\"72\", min_year=2010):\n",
    "    current_year = datetime.now(UTC).year\n",
    "    for year in range(current_year, min_year - 1, -1):\n",
    "        url = f\"https://api.census.gov/data/{year}/{ACS_DATASET}\"\n",
    "        params = {\"get\": \"NAME\", \"for\": f\"state:{state_fips}\"}\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                return year\n",
    "        except requests.RequestException:\n",
    "            continue\n",
    "    raise RuntimeError(\"Unable to find an available ACS 5-year dataset year.\")\n",
    "\n",
    "\n",
    "def census_get(year, variables, geography_params, api_key=None):\n",
    "    url = f\"https://api.census.gov/data/{year}/{ACS_DATASET}\"\n",
    "    params = {\"get\": \",\".join(variables)}\n",
    "    params.update(geography_params)\n",
    "    if api_key:\n",
    "        params[\"key\"] = api_key\n",
    "\n",
    "    response = requests.get(url, params=params, timeout=90)\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "    except requests.HTTPError as exc:\n",
    "        raise RuntimeError(f\"Census API error for {url}: {response.text}\") from exc\n",
    "\n",
    "    payload = response.json()\n",
    "    return pd.DataFrame(payload[1:], columns=payload[0])\n",
    "\n",
    "\n",
    "def geocode_coordinates(lat, lon):\n",
    "    url = \"https://geocoding.geo.census.gov/geocoder/geographies/coordinates\"\n",
    "    params = {\n",
    "        \"x\": lon,\n",
    "        \"y\": lat,\n",
    "        \"benchmark\": \"Public_AR_Current\",\n",
    "        \"vintage\": \"Current_Current\",\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "    response = requests.get(url, params=params, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    result = response.json().get(\"result\", {}).get(\"geographies\", {})\n",
    "\n",
    "    county_info = (result.get(\"Counties\") or [{}])[0]\n",
    "    tract_info = (result.get(\"Census Tracts\") or [{}])[0]\n",
    "\n",
    "    return {\n",
    "        \"county_geoid\": county_info.get(\"GEOID\"),\n",
    "        \"county_name\": county_info.get(\"NAME\"),\n",
    "        \"tract_geoid\": tract_info.get(\"GEOID\"),\n",
    "        \"tract_name\": tract_info.get(\"NAME\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def choose_acs_year():\n",
    "    if ACS_YEAR_ENV:\n",
    "        try:\n",
    "            return int(ACS_YEAR_ENV)\n",
    "        except ValueError as exc:\n",
    "            raise ValueError(\"ACS_YEAR must be an integer like 2024\") from exc\n",
    "    return find_latest_acs5_year(state_fips=PR_STATE_FIPS)\n",
    "\n",
    "\n",
    "selected_year = choose_acs_year()\n",
    "print(f\"ACS year selected: {selected_year}\")\n",
    "print(f\"Census API key provided: {'yes' if CENSUS_API_KEY else 'no'}\")\n",
    "print(f\"Census geocoder enabled: {ENABLE_CENSUS_GEOCODER}\")\n",
    "print(\"Cell 2 complete: config and helpers ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Pull ACS data for PR municipios and ZCTAs\n",
    "requested_columns = [\"NAME\", *ACS_VARIABLES.values()]\n",
    "rename_map = {v: k for k, v in ACS_VARIABLES.items()}\n",
    "\n",
    "# Municipio-level data (Census county geography for Puerto Rico)\n",
    "municipio_df = census_get(\n",
    "    year=selected_year,\n",
    "    variables=requested_columns,\n",
    "    geography_params={\"for\": \"county:*\", \"in\": f\"state:{PR_STATE_FIPS}\"},\n",
    "    api_key=CENSUS_API_KEY,\n",
    ")\n",
    "municipio_df = municipio_df.rename(columns=rename_map)\n",
    "municipio_df = to_numeric(municipio_df, list(ACS_VARIABLES.keys()))\n",
    "municipio_df[\"municipio\"] = (\n",
    "    municipio_df[\"NAME\"]\n",
    "    .str.replace(\", Puerto Rico\", \"\", regex=False)\n",
    "    .str.replace(\" Municipio\", \"\", regex=False)\n",
    ")\n",
    "municipio_df[\"municipio_key\"] = municipio_df[\"municipio\"].map(normalize_text)\n",
    "\n",
    "# ZCTA data is queried nationally, then filtered to PR ZIP codes from local town table\n",
    "zcta_df = census_get(\n",
    "    year=selected_year,\n",
    "    variables=requested_columns,\n",
    "    geography_params={\"for\": \"zip code tabulation area:*\"},\n",
    "    api_key=CENSUS_API_KEY,\n",
    ")\n",
    "zcta_rename = rename_map.copy()\n",
    "zcta_rename[\"zip code tabulation area\"] = \"zip_code\"\n",
    "zcta_df = zcta_df.rename(columns=zcta_rename)\n",
    "zcta_df = to_numeric(zcta_df, list(ACS_VARIABLES.keys()))\n",
    "\n",
    "# Load local town lookup\n",
    "pr_towns_file = resolve_file(DEFAULT_TOWNS_FILE, env_var=\"PR_TOWNS_COORDS_FILE\")\n",
    "pr_towns_df = pd.read_excel(pr_towns_file)\n",
    "if \"municipio2\" in pr_towns_df.columns:\n",
    "    pr_towns_df[\"municipio\"] = pr_towns_df[\"municipio2\"].fillna(pr_towns_df.get(\"municipio\"))\n",
    "elif \"municipio\" not in pr_towns_df.columns:\n",
    "    raise ValueError(\"Town lookup file must include municipio or municipio2 column.\")\n",
    "\n",
    "for col in [\"latitude\", \"longitude\"]:\n",
    "    if col in pr_towns_df.columns:\n",
    "        pr_towns_df[col] = pd.to_numeric(pr_towns_df[col], errors=\"coerce\")\n",
    "\n",
    "if \"Zip Codes\" not in pr_towns_df.columns:\n",
    "    raise ValueError(\"Town lookup file must include 'Zip Codes' column.\")\n",
    "\n",
    "pr_towns_df[\"municipio_key\"] = pr_towns_df[\"municipio\"].map(normalize_text)\n",
    "pr_towns_df[\"zip_list\"] = pr_towns_df[\"Zip Codes\"].map(parse_zip_codes)\n",
    "pr_zip_set = {zip_code for zip_list in pr_towns_df[\"zip_list\"] for zip_code in zip_list}\n",
    "\n",
    "zcta_pr_df = zcta_df[zcta_df[\"zip_code\"].isin(pr_zip_set)].copy()\n",
    "\n",
    "print(f\"Municipios pulled: {len(municipio_df)}\")\n",
    "print(f\"US ZCTAs pulled: {len(zcta_df)}\")\n",
    "print(f\"PR ZCTAs retained after filter: {len(zcta_pr_df)}\")\n",
    "print(f\"Unique PR ZIP codes from lookup: {len(pr_zip_set)}\")\n",
    "\n",
    "print(\"\\nMunicipio sample:\")\n",
    "display(municipio_df.head(3))\n",
    "print(\"\\nZCTA sample:\")\n",
    "display(zcta_pr_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a969bf4-1578-4956-8b1a-ee4e39d7c0ce",
   "metadata": {},
   "source": [
    "## Baseline Census Vulnerability Risk Index (Peer Review Draft)\n",
    "\n",
    "This index is a **baseline vulnerability score** built from ACS Census data.  \n",
    "It does **not** predict flood depth or exact damage by itself. Instead, it ranks places by demographic and social conditions that can make flood impacts harder to absorb and recover from.\n",
    "\n",
    "### What comes from Census vs. what we created\n",
    "- **From Census (ACS 5-year):**\n",
    "  - **Population (`B01003_001E`)**: estimated total number of residents in the area. In the index, this is used as an exposure proxy (how many people could be affected).\n",
    "  - **Median household income (`B19013_001E`)**: the midpoint of household income distribution (half of households earn more, half earn less). Lower income is treated as higher vulnerability in our scoring.\n",
    "  - **Poverty status (`B17001`)**:\n",
    "    - `poverty_universe` (`B17001_001E`): people for whom poverty status is determined.\n",
    "    - `poverty_count` (`B17001_002E`): people below the federal poverty threshold.\n",
    "    - We convert these into **poverty rate = poverty_count / poverty_universe**.\n",
    "  - **Housing occupancy and vacancy (`B25001`, `B25002`)**:\n",
    "    - `housing_units` (`B25001_001E`): total housing units.\n",
    "    - `occupied_units` (`B25002_002E`): units currently occupied.\n",
    "    - `vacant_units` (`B25002_003E`): units currently vacant.\n",
    "    - We convert these into **vacancy rate = vacant_units / housing_units**.\n",
    "  - **Households with no vehicle available (`B25044`)**:\n",
    "    - `no_vehicle_owner` (`B25044_003E`): owner-occupied households with zero vehicles.\n",
    "    - `no_vehicle_renter` (`B25044_010E`): renter-occupied households with zero vehicles.\n",
    "    - We convert these into **no-vehicle rate = (no_vehicle_owner + no_vehicle_renter) / occupied_units**.\n",
    "- **Created by us (method design):**\n",
    "  - **Rate engineering:** We transformed raw counts into proportions (for example, poverty rate, no-vehicle rate, vacancy rate) so places of different sizes can be compared fairly. Counts alone can be misleading because larger places naturally have larger totals.\n",
    "  - **Min-max normalization:** We rescaled each factor to a common `0-1` range using `(x - min) / (max - min)`. This was chosen so variables with different units (people, dollars, housing units, rates) can be combined in one index without any single unit dominating by scale alone.\n",
    "  - **Inverted income-vulnerability transform:** For median income, we apply `1 - minmax(income)` so lower income maps to higher vulnerability. This preserves the direction of all components (higher score always means higher vulnerability) and keeps interpretation consistent.\n",
    "  - **Weighting choices:** We set explicit weights (`0.30, 0.25, 0.20, 0.15, 0.10`) to represent a transparent baseline judgment about relative importance of exposure and vulnerability dimensions. These are intentionally auditable and easy to challenge, tune, and test in sensitivity analysis.\n",
    "  - **Final `0-100` risk score:** We compute the weighted sum in `0-1`, then scale to `0-100` and round to one decimal for readability. This improves communication to technical and non-technical users while keeping the score a **relative index** (not a direct probability of flood damage).\n",
    "- **Important note:** ACS values are survey-based 5-year estimates (with uncertainty), not exact headcounts.\n",
    "\n",
    "### Calculation logic (step-by-step)\n",
    "1. Start with ACS variables for each geography (municipio or ZIP/ZCTA).\n",
    "2. Build rate-based vulnerability features:\n",
    "   - `poverty_rate = poverty_count / poverty_universe`\n",
    "   - `no_vehicle_rate = (no_vehicle_owner + no_vehicle_renter) / occupied_units`\n",
    "   - `vacancy_rate = vacant_units / housing_units`\n",
    "3. Convert each feature to a comparable 0-1 score with min-max scaling:\n",
    "   - `minmax(x) = (x - min) / (max - min)`\n",
    "   - Income is inverted because lower income implies higher vulnerability:\n",
    "   - `income_vulnerability = 1 - minmax(median_income)`\n",
    "4. Combine scores into a weighted baseline index:\n",
    "   - `risk_index_raw = 0.30*population + 0.25*poverty + 0.20*income_vulnerability + 0.15*no_vehicle + 0.10*vacancy`\n",
    "5. Scale to `0-100`:\n",
    "   - `risk_index = round(risk_index_raw * 100, 1)`\n",
    "\n",
    "### Rationale for factors and weights\n",
    "- **Population (0.30):** more people exposed can mean larger potential impact.\n",
    "- **Poverty rate (0.25):** higher poverty is often linked to lower resilience and fewer recovery resources.\n",
    "- **Income vulnerability (0.20):** lower median income can indicate reduced adaptive capacity.\n",
    "- **No-vehicle rate (0.15):** transportation constraints can affect evacuation and access to services.\n",
    "- **Vacancy rate (0.10):** can indicate housing instability or neighborhood stress (a weaker but useful supporting signal).\n",
    "\n",
    "These weights are a **transparent starting hypothesis**, not a final truth. The value is that everyone can inspect and challenge them.\n",
    "\n",
    "### How to interpret the score\n",
    "- Higher score = higher **relative vulnerability** within this dataset.\n",
    "- The score is best used for **prioritization** and **triage**, not as a standalone decision rule.\n",
    "- Compare places within the same run/year carefully; scaling is relative to the included geographies.\n",
    "\n",
    "### Limits to acknowledge in peer review\n",
    "- This is a **social vulnerability baseline**, not a full hazard model.\n",
    "- It does not directly include rainfall intensity, flood depth, terrain, drainage, or infrastructure failure.\n",
    "- Min-max scoring depends on the current sample; extreme values can stretch/compress others.\n",
    "- Missing values are currently handled conservatively (`fillna(0)` in the weighted sum), which may understate risk in sparse-data areas.\n",
    "\n",
    "### \"Index-making eyes\" checklist for students\n",
    "When reviewing or building any index, ask:\n",
    "1. Are the inputs credible and well-defined?\n",
    "2. Do engineered features match the concept (risk/vulnerability/exposure)?\n",
    "3. Is normalization appropriate for cross-variable comparison?\n",
    "4. Are weights justified and testable?\n",
    "5. How sensitive are rankings to small weight/data changes?\n",
    "6. What is missing that could change decisions?\n",
    "7. Is the index explainable to non-technical stakeholders?\n",
    "\n",
    "### Contribution ideas for the class\n",
    "1. Run a sensitivity test (change weights +/-10-20%) and compare rank stability.\n",
    "2. Add hazard layers (rainfall/flood frequency) to separate hazard from vulnerability.\n",
    "3. Test alternate normalization (z-score, quantile scoring) and compare outputs.\n",
    "4. Validate against known past flood impact locations.\n",
    "5. Propose subgroup-specific indexes (e.g., evacuation-focused vs. recovery-focused).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32bb4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Build risk features and baseline risk indexes\n",
    "\n",
    "def add_risk_features(df):\n",
    "    out = df.copy()\n",
    "\n",
    "    out[\"poverty_rate\"] = safe_divide(out[\"poverty_count\"], out[\"poverty_universe\"])\n",
    "    out[\"no_vehicle_rate\"] = safe_divide(\n",
    "        out[\"no_vehicle_owner\"] + out[\"no_vehicle_renter\"],\n",
    "        out[\"occupied_units\"],\n",
    "    )\n",
    "    out[\"vacancy_rate\"] = safe_divide(out[\"vacant_units\"], out[\"housing_units\"])\n",
    "\n",
    "    out[\"score_population\"] = minmax_score(out[\"population\"])\n",
    "    out[\"score_poverty\"] = minmax_score(out[\"poverty_rate\"])\n",
    "    out[\"score_income_vulnerability\"] = minmax_score(out[\"median_income\"], invert=True)\n",
    "    out[\"score_transport_vulnerability\"] = minmax_score(out[\"no_vehicle_rate\"])\n",
    "    out[\"score_housing_vulnerability\"] = minmax_score(out[\"vacancy_rate\"])\n",
    "\n",
    "    # Weighted baseline index (0-100). Adjust weights as your model evolves.\n",
    "    out[\"risk_index_raw\"] = (\n",
    "        0.30 * out[\"score_population\"].fillna(0)\n",
    "        + 0.25 * out[\"score_poverty\"].fillna(0)\n",
    "        + 0.20 * out[\"score_income_vulnerability\"].fillna(0)\n",
    "        + 0.15 * out[\"score_transport_vulnerability\"].fillna(0)\n",
    "        + 0.10 * out[\"score_housing_vulnerability\"].fillna(0)\n",
    "    )\n",
    "    out[\"risk_index\"] = (out[\"risk_index_raw\"] * 100).round(1)\n",
    "    return out\n",
    "\n",
    "\n",
    "municipio_risk_df = add_risk_features(municipio_df)\n",
    "zcta_risk_df = add_risk_features(zcta_pr_df)\n",
    "\n",
    "print(\"Top municipio risk rows:\")\n",
    "display(\n",
    "    municipio_risk_df[[\n",
    "        \"municipio\", \"population\", \"median_income\", \"poverty_rate\", \"no_vehicle_rate\", \"risk_index\"\n",
    "    ]]\n",
    "    .sort_values(\"risk_index\", ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "print(\"Top ZCTA risk rows:\")\n",
    "display(\n",
    "    zcta_risk_df[[\n",
    "        \"zip_code\", \"population\", \"median_income\", \"poverty_rate\", \"no_vehicle_rate\", \"risk_index\"\n",
    "    ]]\n",
    "    .sort_values(\"risk_index\", ascending=False)\n",
    "    .head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cece5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Join municipio + ZIP features to town coordinates (model-ready table)\n",
    "\n",
    "towns_exploded_df = (\n",
    "    pr_towns_df[[\"designated_area\", \"municipio\", \"municipio_key\", \"latitude\", \"longitude\", \"zip_list\"]]\n",
    "    .explode(\"zip_list\")\n",
    "    .rename(columns={\"zip_list\": \"zip_code\"})\n",
    ")\n",
    "\n",
    "zip_feature_cols = [\n",
    "    \"zip_code\", \"risk_index\", \"population\", \"median_income\", \"poverty_rate\", \"no_vehicle_rate\", \"vacancy_rate\"\n",
    "]\n",
    "zip_aggregated_df = (\n",
    "    towns_exploded_df\n",
    "    .merge(zcta_risk_df[zip_feature_cols], on=\"zip_code\", how=\"left\")\n",
    "    .groupby([\"designated_area\", \"municipio\", \"municipio_key\", \"latitude\", \"longitude\"], dropna=False)\n",
    "    .agg(\n",
    "        zip_count=(\"zip_code\", \"nunique\"),\n",
    "        zip_risk_index=(\"risk_index\", \"mean\"),\n",
    "        zip_population=(\"population\", \"sum\"),\n",
    "        zip_median_income=(\"median_income\", \"mean\"),\n",
    "        zip_poverty_rate=(\"poverty_rate\", \"mean\"),\n",
    "        zip_no_vehicle_rate=(\"no_vehicle_rate\", \"mean\"),\n",
    "        zip_vacancy_rate=(\"vacancy_rate\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "municipio_feature_cols = [\n",
    "    \"municipio_key\", \"risk_index\", \"population\", \"median_income\", \"poverty_rate\", \"no_vehicle_rate\", \"vacancy_rate\"\n",
    "]\n",
    "town_risk_df = zip_aggregated_df.merge(\n",
    "    municipio_risk_df[municipio_feature_cols].rename(\n",
    "        columns={\n",
    "            \"risk_index\": \"municipio_risk_index\",\n",
    "            \"population\": \"municipio_population\",\n",
    "            \"median_income\": \"municipio_median_income\",\n",
    "            \"poverty_rate\": \"municipio_poverty_rate\",\n",
    "            \"no_vehicle_rate\": \"municipio_no_vehicle_rate\",\n",
    "            \"vacancy_rate\": \"municipio_vacancy_rate\",\n",
    "        }\n",
    "    ),\n",
    "    on=\"municipio_key\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "\n",
    "def blend_risk(municipio_risk, zip_risk):\n",
    "    if pd.notna(municipio_risk) and pd.notna(zip_risk):\n",
    "        return round(0.6 * municipio_risk + 0.4 * zip_risk, 1)\n",
    "    if pd.notna(municipio_risk):\n",
    "        return round(municipio_risk, 1)\n",
    "    if pd.notna(zip_risk):\n",
    "        return round(zip_risk, 1)\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "town_risk_df[\"risk_index\"] = town_risk_df.apply(\n",
    "    lambda row: blend_risk(row[\"municipio_risk_index\"], row[\"zip_risk_index\"]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Optional: enrich points with county/tract GEOIDs for coordinate-based joins\n",
    "if ENABLE_CENSUS_GEOCODER:\n",
    "    geocoded_records = []\n",
    "    for row in town_risk_df.itertuples(index=False):\n",
    "        if pd.notna(row.latitude) and pd.notna(row.longitude):\n",
    "            try:\n",
    "                geocoded_records.append(geocode_coordinates(row.latitude, row.longitude))\n",
    "            except Exception as exc:\n",
    "                logger.warning(\"Geocoder failed for %s: %s\", row.designated_area, exc)\n",
    "                geocoded_records.append({\n",
    "                    \"county_geoid\": None,\n",
    "                    \"county_name\": None,\n",
    "                    \"tract_geoid\": None,\n",
    "                    \"tract_name\": None,\n",
    "                })\n",
    "        else:\n",
    "            geocoded_records.append({\n",
    "                \"county_geoid\": None,\n",
    "                \"county_name\": None,\n",
    "                \"tract_geoid\": None,\n",
    "                \"tract_name\": None,\n",
    "            })\n",
    "\n",
    "    geocoded_df = pd.DataFrame(geocoded_records)\n",
    "    town_risk_df = pd.concat([town_risk_df.reset_index(drop=True), geocoded_df], axis=1)\n",
    "\n",
    "print(\"Town-level sample:\")\n",
    "display(\n",
    "    town_risk_df[[\n",
    "        \"designated_area\", \"municipio\", \"latitude\", \"longitude\", \"risk_index\",\n",
    "        \"municipio_risk_index\", \"zip_risk_index\", \"zip_count\"\n",
    "    ]].head(10)\n",
    ")\n",
    "\n",
    "print(f\"Town rows generated: {len(town_risk_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51029638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Export CSV + GeoJSON outputs\n",
    "municipio_out = OUTPUT_DIR / \"municipio_risk_features.csv\"\n",
    "zcta_out = OUTPUT_DIR / \"zcta_risk_features.csv\"\n",
    "town_out = OUTPUT_DIR / \"town_risk_features.csv\"\n",
    "geojson_out = OUTPUT_DIR / \"town_risk_features.geojson\"\n",
    "\n",
    "municipio_risk_df.to_csv(municipio_out, index=False)\n",
    "zcta_risk_df.to_csv(zcta_out, index=False)\n",
    "town_risk_df.to_csv(town_out, index=False)\n",
    "\n",
    "geojson_features = []\n",
    "for row in town_risk_df.to_dict(orient=\"records\"):\n",
    "    lat = row.get(\"latitude\")\n",
    "    lon = row.get(\"longitude\")\n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        continue\n",
    "\n",
    "    attrs = {}\n",
    "    for key, value in row.items():\n",
    "        if pd.isna(value):\n",
    "            attrs[key] = None\n",
    "        elif isinstance(value, (np.integer, np.floating)):\n",
    "            attrs[key] = value.item()\n",
    "        else:\n",
    "            attrs[key] = value\n",
    "\n",
    "    geojson_features.append(\n",
    "        {\n",
    "            \"type\": \"Feature\",\n",
    "            \"geometry\": {\"type\": \"Point\", \"coordinates\": [float(lon), float(lat)]},\n",
    "            \"properties\": attrs,\n",
    "        }\n",
    "    )\n",
    "\n",
    "geojson_payload = {\"type\": \"FeatureCollection\", \"features\": geojson_features}\n",
    "with open(geojson_out, \"w\", encoding=\"utf-8\") as file_handle:\n",
    "    json.dump(geojson_payload, file_handle, ensure_ascii=False, indent=2, default=str)\n",
    "\n",
    "print(\"Export complete:\")\n",
    "print(f\"- {municipio_out}\")\n",
    "print(f\"- {zcta_out}\")\n",
    "print(f\"- {town_out}\")\n",
    "print(f\"- {geojson_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f06ff6",
   "metadata": {},
   "source": [
    "## Next Modeling Steps\n",
    "\n",
    "- Add hazard intensity features from your USGS/NOAA/USACE notebooks (flood stage, alert density, event frequency).\n",
    "- Use this notebook output as static/demographic vulnerability features.\n",
    "- Train and compare candidate models (e.g., linear baseline, random forest, gradient boosting) on a shared event-labeled dataset.\n",
    "- Keep all secrets in `.env` only; do not hardcode keys in notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdbed29-81d0-46f8-9e88-550d98b9f4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
