{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to your notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this cell to connect to your GIS and get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from arcgis.gis import GIS\n",
    "# import contextlib, io\n",
    "# with contextlib.redirect_stderr(io.StringIO()):\n",
    "#     gis = GIS(\"home\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now you are ready to start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ### NWS Point Forecast to ArcGIS Feature Layer Updater ###\n",
    "#\n",
    "# Optional: ArcGIS Online Sync\n",
    "# To publish updates to ArcGIS Online, set:\n",
    "#   USE_ARCGIS=1\n",
    "#   NWS_FORECAST_LAYER_ID (or FEATURE_LAYER_ITEM_ID)\n",
    "# Then re-run the notebook from Cell 1.\n",
    "# =============================================================================\n",
    "# This script is designed to run in an ArcGIS Online Notebook environment.\n",
    "# It fetches the latest 7-day weather forecast from the National Weather\n",
    "# Service (NWS) API for a specific point and updates a corresponding\n",
    "# Feature Layer in ArcGIS Online.\n",
    "#\n",
    "# --- PROCESS ---\n",
    "# 1.  Connects to the ArcGIS Online organization (\"home\").\n",
    "# 2.  Defines file paths for the configuration files located on the AGOL server.\n",
    "# 3.  Reads 'PR Alert Data Sources.xlsx' to get the NWS API endpoint and\n",
    "#     the latitude/longitude from the 'Parameters_or_Selectors' column.\n",
    "# 4.  Performs the two-step NWS API request to get the forecast data.\n",
    "# 5.  Processes the JSON response into a pandas DataFrame.\n",
    "# 6.  Converts datetime fields to a string format compatible with AGOL.\n",
    "# 7.  Iterates through the DataFrame to create a list of arcgis.features.Feature\n",
    "#     objects, manually creating the point geometry for each.\n",
    "# 8.  Connects to the target Feature Layer using its Item ID.\n",
    "# 9.  Deletes all existing records in the layer.\n",
    "# 10. Appends the new forecast records to the Feature Layer.\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set USE_ARCGIS=1 to enable ArcGIS Online sync; otherwise run locally.\n",
    "USE_ARCGIS = os.environ.get(\"USE_ARCGIS\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
    "\n",
    "if USE_ARCGIS:\n",
    "    from arcgis.gis import GIS\n",
    "    from arcgis.features import FeatureLayer, Feature\n",
    "    from arcgis.geometry import Point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 0: Configure Logging ---\n",
    "# Helper: resolve local files without hardcoding machine-specific paths\n",
    "def resolve_file(filename, env_var=None, search_roots=None):\n",
    "    if env_var:\n",
    "        env_val = os.environ.get(env_var)\n",
    "        if env_val:\n",
    "            return env_val\n",
    "    roots = search_roots or [Path.cwd(), Path.cwd().parent, Path.home()]\n",
    "    arcgis_home = Path(\"/arcgis/home\")\n",
    "    if arcgis_home.exists():\n",
    "        roots.append(arcgis_home)\n",
    "    for root in roots:\n",
    "        if root.exists():\n",
    "            match = next(root.rglob(filename), None)\n",
    "            if match:\n",
    "                return str(match)\n",
    "    raise FileNotFoundError(\"Set the required env var or place the file under the repo or /arcgis/home.\")\n",
    "\n",
    "# Set up logging to provide detailed, play-by-play output\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\", stream=sys.stdout)\n",
    "\n",
    "# --- ArcGIS Online Configuration (optional) ---\n",
    "if USE_ARCGIS:\n",
    "    logging.info(\"--- Connecting to ArcGIS Online ---\")\n",
    "    try:\n",
    "        gis = GIS(\"home\")\n",
    "        logging.info(\"Connected to ArcGIS Online.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ FATAL ERROR: Could not connect to ArcGIS Online. Reason: {e}\")\n",
    "        sys.exit()\n",
    "else:\n",
    "    gis = None\n",
    "    logging.info(\"ArcGIS disabled; running locally only.\")\n",
    "\n",
    "# --- Configuration ---\n",
    "ITEM_ID = os.environ.get('NWS_FORECAST_LAYER_ID') or os.environ.get('FEATURE_LAYER_ITEM_ID')\n",
    "LAYER_COUNT = 0\n",
    "\n",
    "if USE_ARCGIS and not ITEM_ID:\n",
    "    raise ValueError(\"Set NWS_FORECAST_LAYER_ID (or FEATURE_LAYER_ITEM_ID) in the environment.\")\n",
    "\n",
    "CONFIG_FILE_NAME = resolve_file('PR Alert Data Sources.xlsx', env_var='PR_ALERT_XLSX')\n",
    "TARGET_SOURCE_NAME = 'NWS - Point Forecast Lookup'\n",
    "\n",
    "# Local outputs (for non-ArcGIS runs)\n",
    "OUTPUT_DIR = Path(os.environ.get(\"OUTPUT_DIR\", \"outputs\"))\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_CSV = OUTPUT_DIR / \"nws_forecast.csv\"\n",
    "OUTPUT_GEOJSON = OUTPUT_DIR / \"nws_forecast.geojson\"\n",
    "\n",
    "logging.info(\"--- Configuration ---\")\n",
    "logging.info(\"  > Config file resolved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Load the configuration file ---\n",
    "logging.info(\"\\n--- Reading Configuration File ---\")\n",
    "try:\n",
    "    config_df = pd.read_excel(CONFIG_FILE_NAME)\n",
    "    nws_source_row = config_df[config_df['Source_Name'] == TARGET_SOURCE_NAME]\n",
    "\n",
    "    if nws_source_row.empty:\n",
    "        logging.error(f\"❌ FATAL ERROR: Could not find a source named '{TARGET_SOURCE_NAME}' in '{CONFIG_FILE_NAME}'.\")\n",
    "        sys.exit()\n",
    "\n",
    "    source_details = nws_source_row.iloc[0]\n",
    "    logging.info(\"✅ Successfully found API configuration.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"FATAL ERROR: Configuration file not found. Set PR_ALERT_XLSX or place PR Alert Data Sources.xlsx under the repo or /arcgis/home.\")\n",
    "    sys.exit()\n",
    "except Exception as e:\n",
    "    logging.error(f\"❌ FATAL ERROR: An error occurred while reading the Excel file: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- Step 2: Extract details from the configuration ---\n",
    "endpoint_url = source_details['URL_Endpoint']\n",
    "params_or_selectors = source_details['Parameters_or_Selectors']\n",
    "lat, lon = None, None\n",
    "\n",
    "try:\n",
    "    params = json.loads(params_or_selectors)\n",
    "    lat = params.get('lat')\n",
    "    lon = params.get('lon')\n",
    "\n",
    "    if lat is None or lon is None:\n",
    "        logging.error(\"❌ FATAL ERROR: 'lat' and 'lon' not found in the 'Parameters_or_Selectors' column.\")\n",
    "        sys.exit()\n",
    "\n",
    "    logging.info(f\"  > Latitude: {lat}\")\n",
    "    logging.info(f\"  > Longitude: {lon}\")\n",
    "\n",
    "except (json.JSONDecodeError, TypeError):\n",
    "    logging.error(\"FATAL ERROR: Could not parse Parameters_or_Selectors as JSON.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- Step 3: Perform the two-step NWS API request ---\n",
    "all_features = []\n",
    "headers = {\n",
    "    'User-Agent': os.environ.get('NWS_USER_AGENT', 'DAEN-NWS-Notebook/1.0 (contact)'),\n",
    "    'Accept': 'application/geo+json'\n",
    "}\n",
    "\n",
    "logging.info(\"\\n--- Starting NWS API Fetch ---\")\n",
    "try:\n",
    "    point_lookup_url = f\"{endpoint_url.strip('/')}/{lat},{lon}\"\n",
    "    logging.info(f\"  > [Step 1] Requesting point lookup: {point_lookup_url}\")\n",
    "    response_step1 = requests.get(point_lookup_url, headers=headers, timeout=30)\n",
    "    response_step1.raise_for_status()\n",
    "    data_step1 = response_step1.json()\n",
    "    forecast_url = data_step1.get('properties', {}).get('forecast')\n",
    "\n",
    "    if not forecast_url:\n",
    "        logging.error(\"❌ FATAL ERROR: Could not find a 'forecast' URL in the point lookup response.\")\n",
    "        sys.exit()\n",
    "\n",
    "    logging.info(f\"  > [Step 2] Fetching forecast data from: {forecast_url}\")\n",
    "    response_step2 = requests.get(forecast_url, headers=headers, timeout=30)\n",
    "    response_step2.raise_for_status()\n",
    "    data_step2 = response_step2.json()\n",
    "    all_features = data_step2.get('properties', {}).get('periods', [])\n",
    "    logging.info(f\"✅ Successfully fetched {len(all_features)} forecast periods.\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    logging.error(f\"❌ FATAL ERROR: Could not fetch data from NWS API. Reason: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- Step 4: Process data into a DataFrame ---\n",
    "logging.info(\"\\n--- Processing Data into DataFrame ---\")\n",
    "if all_features:\n",
    "    records = []\n",
    "    for period in all_features:\n",
    "        start_time_dt = pd.to_datetime(period.get('startTime'))\n",
    "        records.append({\n",
    "            'latitude': lat,\n",
    "            'longitude': lon,\n",
    "            'date': start_time_dt,\n",
    "            'period': period.get('name'),\n",
    "            'startTime': start_time_dt,\n",
    "            'temp': f\"{period.get('temperature')} {period.get('temperatureUnit')}\",\n",
    "            'wind': f\"{period.get('windSpeed')} {period.get('windDirection')}\",\n",
    "            'forecast_short': period.get('shortForecast'),\n",
    "            'forecast_detailed': period.get('detailedForecast')\n",
    "        })\n",
    "    df = pd.DataFrame(records)\n",
    "    logging.info(f\"✅ DataFrame created with {len(df)} records.\")\n",
    "else:\n",
    "    logging.warning(\"❌ No forecast periods found. Cannot update Feature Layer. Exiting.\")\n",
    "    sys.exit()\n",
    "\n",
    "if not USE_ARCGIS:\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "    def to_jsonable(val):\n",
    "        if isinstance(val, pd.Timestamp):\n",
    "            return val.isoformat()\n",
    "        try:\n",
    "            if pd.isna(val):\n",
    "                return None\n",
    "        except Exception:\n",
    "            pass\n",
    "        if hasattr(val, \"item\"):\n",
    "            try:\n",
    "                return val.item()\n",
    "            except Exception:\n",
    "                pass\n",
    "        return val\n",
    "\n",
    "    features = []\n",
    "    if {\"longitude\", \"latitude\"}.issubset(df.columns):\n",
    "        for _, row in df.iterrows():\n",
    "            lon = row.get(\"longitude\")\n",
    "            lat = row.get(\"latitude\")\n",
    "            if pd.notna(lon) and pd.notna(lat):\n",
    "                props = row.drop(labels=[\"longitude\", \"latitude\"]).to_dict()\n",
    "                props = {k: to_jsonable(v) for k, v in props.items()}\n",
    "                features.append({\n",
    "                    \"type\": \"Feature\",\n",
    "                    \"geometry\": {\"type\": \"Point\", \"coordinates\": [float(lon), float(lat)]},\n",
    "                    \"properties\": props\n",
    "                })\n",
    "    geojson = {\"type\": \"FeatureCollection\", \"features\": features}\n",
    "    with open(OUTPUT_GEOJSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(geojson, f, ensure_ascii=False, indent=2)\n",
    "    logging.info(\"Local outputs written: %s and %s\", OUTPUT_CSV, OUTPUT_GEOJSON)\n",
    "\n",
    "\n",
    "if USE_ARCGIS:\n",
    "    # --- Step 5: Update the ArcGIS Online Feature Layer ---\n",
    "    logging.info(\"\\n--- Updating ArcGIS Online Feature Layer ---\")\n",
    "    try:\n",
    "        logging.info(\"  > Getting Feature Layer Item\")\n",
    "        feature_layer_item = gis.content.get(ITEM_ID)\n",
    "    \n",
    "        if not feature_layer_item:\n",
    "            logging.error(\"FATAL ERROR: Could not find the feature layer item. Check ITEM_ID / env vars.\")\n",
    "            sys.exit()\n",
    "\n",
    "        target_layer = feature_layer_item.layers[LAYER_COUNT]\n",
    "        logging.info(f\"  > Accessing layer: '{target_layer.properties.name}'\")\n",
    "\n",
    "        logging.info(\"  > Clearing all existing records from the layer...\")\n",
    "        target_layer.delete_features(where=\"1=1\")\n",
    "        logging.info(\"  > Layer successfully cleared.\")\n",
    "\n",
    "        # --- Step 6: Prepare features for upload ---\n",
    "        logging.info(\"\\n--- Preparing Features for Upload ---\")\n",
    "    \n",
    "        # FIX: Convert datetime objects to a standard string format before upload.\n",
    "        # This prevents the 'Int64 to DateTime' conversion error.\n",
    "        logging.info(\"  > Converting datetime columns to string format for AGOL compatibility...\")\n",
    "        df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        df['startTime'] = df['startTime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "        features_to_add = []\n",
    "        for index, row in df.iterrows():\n",
    "            attributes = row.to_dict()\n",
    "        \n",
    "            # Clean up potential null values before creating the feature\n",
    "            clean_attrs = {k: v for k, v in attributes.items() if pd.notna(v)}\n",
    "        \n",
    "            geometry = Point({\n",
    "                \"x\": row['longitude'],\n",
    "                \"y\": row['latitude'],\n",
    "                \"spatialReference\": {\"wkid\": 4326}\n",
    "            })\n",
    "        \n",
    "            new_feature = Feature(geometry=geometry, attributes=clean_attrs)\n",
    "            features_to_add.append(new_feature)\n",
    "    \n",
    "        logging.info(f\"✅ Prepared {len(features_to_add)} features for upload.\")\n",
    "\n",
    "        if features_to_add:\n",
    "            logging.info(f\"  > Appending {len(features_to_add)} new records to the layer...\")\n",
    "            add_result = target_layer.edit_features(adds=features_to_add)\n",
    "        \n",
    "            if add_result['addResults'] and all(res['success'] for res in add_result['addResults']):\n",
    "                logging.info(f\"✅✅✅ Update Complete! Successfully added {len(add_result['addResults'])} records.\")\n",
    "            else:\n",
    "                logging.error(\"❌ FAILED to update Feature Layer. See details below:\")\n",
    "                logging.error(add_result)\n",
    "        else:\n",
    "            logging.info(\"  > No features to add.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ FATAL ERROR: An unexpected error occurred during the AGOL update. Reason: {e}\")\n",
    "\n",
    "    logging.info(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "esriNotebookRuntime": {
   "notebookRuntimeName": "ArcGIS Notebook Python 3 Standard",
   "notebookRuntimeVersion": "12.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
