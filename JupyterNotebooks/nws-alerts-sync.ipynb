{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to your notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this cell to connect to your GIS and get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from arcgis.gis import GIS\n",
    "# import contextlib, io\n",
    "# with contextlib.redirect_stderr(io.StringIO()):\n",
    "#     gis = GIS(\"home\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now you are ready to start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Title: NWS Active Weather Alerts to ArcGIS Feature Layer (Notebook Version)\n",
    "\n",
    "# GitHub: https://github.com/yagaC64/Spring2026DAEN\n",
    "\n",
    "# License: https://github.com/yagaC64/Spring2026DAEN/blob/main/LICENSE\n",
    "\n",
    "# Description: A script for an ArcGIS Notebook that fetches active NWS weather\n",
    "#              alerts for Puerto Rico, processes them, and updates a target\n",
    "#              ArcGIS Feature Layer using a Truncate and Add workflow.\n",
    "# Version: 2.8\n",
    "#\n",
    "# Optional: ArcGIS Online Sync\n",
    "# If you want to publish updates to ArcGIS Online, set:\n",
    "#   USE_ARCGIS=1\n",
    "#   NWS_ALERTS_LAYER_ID (or FEATURE_LAYER_ITEM_ID)\n",
    "# Then re-run the notebook from Cell 1.\n",
    "# =============================================================================\n",
    "# What's new in v2.8 (Definitive Geocoding Fix):\n",
    "# - Reworked the API fallback logic to prevent coordinate overwriting on\n",
    "#   exploded DataFrames.\n",
    "# - The script now caches new coordinates and maps them back in a single,\n",
    "#   safe operation, ensuring distinct coordinates are preserved.\n",
    "# =============================================================================\n",
    "\n",
    "# Cell 1: Install and Import Libraries\n",
    "# =============================================================================\n",
    "import sys\n",
    "import subprocess\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "import os  # Added for file diagnostics\n",
    "from pathlib import Path\n",
    "\n",
    "# Set USE_ARCGIS=1 to enable ArcGIS Online sync; otherwise run locally.\n",
    "USE_ARCGIS = os.environ.get(\"USE_ARCGIS\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
    "\n",
    "# Install required libraries in the notebook environment\n",
    "print(\"Installing required libraries...\")\n",
    "base_pkgs = ['pandas', 'openpyxl', 'requests']\n",
    "if USE_ARCGIS:\n",
    "    base_pkgs.append('arcgis')\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', *base_pkgs])\n",
    "print(\"Installation complete.\")\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "if USE_ARCGIS:\n",
    "    from arcgis.gis import GIS\n",
    "    from arcgis.features import Feature\n",
    "    from arcgis.geometry import Point, Polygon\n",
    "\n",
    "# Import display for rich DataFrame output in notebooks\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except ImportError:\n",
    "    display = print  # Fallback for non-IPython environments\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\", stream=sys.stdout)\n",
    "\n",
    "print(\"\\nCell 1: Libraries installed and imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "def resolve_file(filename, env_var=None, search_roots=None):\n",
    "    if env_var:\n",
    "        env_val = os.environ.get(env_var)\n",
    "        if env_val:\n",
    "            return env_val\n",
    "    roots = search_roots or [Path.cwd(), Path.cwd().parent, Path.home()]\n",
    "    arcgis_home = Path(\"/arcgis/home\")\n",
    "    if arcgis_home.exists():\n",
    "        roots.append(arcgis_home)\n",
    "    for root in roots:\n",
    "        if root.exists():\n",
    "            match = next(root.rglob(filename), None)\n",
    "            if match:\n",
    "                return str(match)\n",
    "    raise FileNotFoundError(\"Set the required env var or place the file under the repo or /arcgis/home.\")\n",
    "\n",
    "# =============================================================================\n",
    "# --- USER-DEFINED VARIABLES ---\n",
    "\n",
    "FEATURE_LAYER_ITEM_ID = os.environ.get(\"NWS_ALERTS_LAYER_ID\") or os.environ.get(\"FEATURE_LAYER_ITEM_ID\")\n",
    "LAYER_INDEX = 0\n",
    "\n",
    "if USE_ARCGIS and not FEATURE_LAYER_ITEM_ID:\n",
    "    raise ValueError(\"Set NWS_ALERTS_LAYER_ID (or FEATURE_LAYER_ITEM_ID) in the environment.\")\n",
    "\n",
    "# Local file paths (resolved dynamically)\n",
    "GEOCODER_FILE = resolve_file(\"puerto_rico_geocoder_reference.xlsx\", env_var=\"PR_GEOCODER_XLSX\")\n",
    "\n",
    "# Local outputs (for non-ArcGIS runs)\n",
    "OUTPUT_DIR = Path(os.environ.get(\"OUTPUT_DIR\", \"outputs\"))\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_CSV = OUTPUT_DIR / \"nws_alerts.csv\"\n",
    "OUTPUT_GEOJSON = OUTPUT_DIR / \"nws_alerts.geojson\"\n",
    "\n",
    "# NWS API Configuration\n",
    "ALERTS_URL = \"https://api.weather.gov/alerts/active\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": os.environ.get(\"NWS_USER_AGENT\", \"DAEN-NWS-Notebook/1.0 (contact)\"),\n",
    "    \"Accept\": \"application/geo+json\"\n",
    "}\n",
    "\n",
    "print(\"Cell 2: Configuration variables set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Connect to ArcGIS Organization (optional)\n",
    "# =============================================================================\n",
    "if USE_ARCGIS:\n",
    "    try:\n",
    "        logging.info(\"Connecting to ArcGIS environment...\")\n",
    "        gis = GIS(\"home\")\n",
    "        logging.info(\"Successfully connected to %s.\", gis.properties.portalHostname)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"FATAL: Failed to connect to ArcGIS. Error: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"Cell 3: ArcGIS connection established.\")\n",
    "else:\n",
    "    gis = None\n",
    "    logging.info(\"ArcGIS disabled; running locally only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Fetch NWS Alerts\n",
    "# =============================================================================\n",
    "# This cell contains the logic to fetch and process NWS alerts.\n",
    "\n",
    "# --- 4.1: Build Puerto Rico Zone Map ---\n",
    "pr_zone_map = {}\n",
    "logging.info(\"--- Building Puerto Rico Zone Map ---\")\n",
    "try:\n",
    "    # This map is crucial for translating zone IDs (e.g., PRZ001) to names (\"San Juan and Vicinity\")\n",
    "    zones_url = \"https://api.weather.gov/zones?area=PR&type=forecast\"\n",
    "    r = requests.get(zones_url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    pr_zones = r.json().get(\"features\", [])\n",
    "    for zone in pr_zones:\n",
    "        zone_id = zone.get(\"properties\", {}).get(\"id\")\n",
    "        zone_name = zone.get(\"properties\", {}).get(\"name\")\n",
    "        if zone_id and zone_name:\n",
    "            pr_zone_map[zone_id] = zone_name\n",
    "    logging.info(f\"Successfully mapped {len(pr_zone_map)} PR forecast zones.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Could not build PR zone map, using fallbacks. Reason: {e}\")\n",
    "    if not pr_zone_map:\n",
    "        for i in range(1, 14): pr_zone_map[f\"PRZ{i:03d}\"] = f\"Puerto Rico Zone {i:03d}\"\n",
    "\n",
    "# --- 4.2: Fetch Data ---\n",
    "all_features = []\n",
    "logging.info(\"\\n--- Starting Data Fetch ---\")\n",
    "# Strategy 1: Fetch all active US alerts\n",
    "logging.info(f\"Fetching all active US alerts from → {ALERTS_URL}\")\n",
    "try:\n",
    "    r = requests.get(ALERTS_URL, headers=HEADERS, timeout=90)\n",
    "    r.raise_for_status()\n",
    "    nationwide_features = r.json().get(\"features\", [])\n",
    "    all_features.extend(nationwide_features)\n",
    "    logging.info(f\"Discovered {len(nationwide_features)} total active alerts nationwide.\")\n",
    "except Exception as e:\n",
    "    logging.warning(f\"Failed to fetch nationwide alerts → {e}\")\n",
    "\n",
    "# Strategy 2: Fetch alerts for specific Puerto Rico zones\n",
    "logging.info(\"\\nFetching alerts for all known PR zones...\")\n",
    "for zone_id in pr_zone_map.keys():\n",
    "    zone_url = f\"https://api.weather.gov/alerts/active/zone/{zone_id}\"\n",
    "    try:\n",
    "        r = requests.get(zone_url, headers=HEADERS, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        zone_features = r.json().get(\"features\", [])\n",
    "        if zone_features: all_features.extend(zone_features)\n",
    "    except Exception:\n",
    "        pass # Failures are expected for zones with no alerts\n",
    "\n",
    "logging.info(\"\\n--- Data Fetch Complete ---\")\n",
    "\n",
    "# --- 4.3: Deduplicate and Filter ---\n",
    "unique_features = {f.get(\"id\"): f for f in all_features if f.get(\"id\")}.values()\n",
    "logging.info(f\"Total unique features scraped → {len(unique_features)}\")\n",
    "pr_alerts = []\n",
    "if unique_features:\n",
    "    pr_pattern = re.compile(r\"puerto\\s+rico|PRZ\\d{3}|AMZ\\d{3}|\\bPR\\b\", re.I)\n",
    "    text_search_fields = [\"areaDesc\", \"headline\", \"description\"]\n",
    "\n",
    "    for feature in unique_features:\n",
    "        properties = feature.get(\"properties\", {})\n",
    "        is_sanjuan_sender = properties.get(\"senderName\") == \"NWS San Juan PR\"\n",
    "        mentions_pr = any(pr_pattern.search(str(properties.get(key, \"\"))) for key in text_search_fields)\n",
    "\n",
    "        if is_sanjuan_sender or mentions_pr:\n",
    "            pr_alerts.append(feature)\n",
    "\n",
    "logging.info(f\"Found {len(pr_alerts)} alerts related to Puerto Rico after filtering.\")\n",
    "\n",
    "print(\"\\nCell 4: NWS Alert data fetching and processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Build and Geocode DataFrame\n",
    "# =============================================================================\n",
    "logging.info(\"--- Building and Geocoding DataFrame ---\")\n",
    "all_records = []\n",
    "found_zone_ids = set()\n",
    "\n",
    "# Process real alerts\n",
    "if pr_alerts:\n",
    "    for feature in pr_alerts:\n",
    "        p = feature.get(\"properties\", {})\n",
    "        affected_zones = [url.split('/')[-1] for url in p.get(\"affectedZones\", [])]\n",
    "        if not affected_zones:\n",
    "            affected_zones = [p.get(\"areaDesc\")]\n",
    "        \n",
    "        found_zone_ids.update(affected_zones)\n",
    "        all_records.append({\n",
    "            \"sent\": p.get(\"sent\"), \"event\": p.get(\"event\"), \"headline\": p.get(\"headline\"),\n",
    "            \"description\": p.get(\"description\"), \"instruction\": p.get(\"instruction\"),\n",
    "            \"response\": p.get(\"response\"), \"severity\": p.get(\"severity\"),\n",
    "            \"urgency\": p.get(\"urgency\"), \"certainty\": p.get(\"certainty\"),\n",
    "            \"expires\": p.get(\"expires\"), \"status\": p.get(\"status\"), \"id\": p.get(\"id\"),\n",
    "            \"affected_zones\": affected_zones\n",
    "        })\n",
    "\n",
    "# Add placeholders for zones with no alerts\n",
    "time_of_check = datetime.now(timezone.utc)\n",
    "for zone_id, zone_name in pr_zone_map.items():\n",
    "    if zone_id not in found_zone_ids:\n",
    "        all_records.append({\n",
    "            \"sent\": time_of_check.isoformat(), \"event\": \"No Active Alerts\",\n",
    "            \"headline\": f\"No Active Alerts for {zone_name}\", \"description\": \"N/A\",\n",
    "            \"instruction\": \"N/A\", \"response\": \"N/A\", \"severity\": \"N/A\",\n",
    "            \"urgency\": \"N/A\", \"certainty\": \"N/A\",\n",
    "            \"expires\": None, \"status\": \"None\",\n",
    "            \"id\": f\"placeholder_{zone_id}_{time_of_check.isoformat()}\",\n",
    "            \"affected_zones\": [zone_id]\n",
    "        })\n",
    "\n",
    "if all_records:\n",
    "    alerts_df = pd.DataFrame(all_records)\n",
    "    alerts_df = alerts_df.explode('affected_zones').rename(columns={'affected_zones': 'zone_id'})\n",
    "    logging.info(f\"DataFrame exploded. Total records now: {len(alerts_df)}\")\n",
    "\n",
    "    alerts_df['sent'] = pd.to_datetime(alerts_df['sent'], errors='coerce')\n",
    "    alerts_df['expires'] = pd.to_datetime(alerts_df['expires'], errors='coerce')\n",
    "    alerts_df['area_desc'] = alerts_df['zone_id'].map(pr_zone_map).fillna(alerts_df['zone_id'])\n",
    "else:\n",
    "    alerts_df = pd.DataFrame()\n",
    "\n",
    "# --- FINAL GEOCODING SWEEP ---\n",
    "if not alerts_df.empty:\n",
    "    logging.info(\"--- STARTING FINAL GEOCODING SWEEP ---\")\n",
    "    # Diagnostics: Check if file exists\n",
    "    if os.path.exists(GEOCODER_FILE):\n",
    "        print(\"Geocoder file found.\")\n",
    "    else:\n",
    "        print(\"Geocoder file not found. Set PR_GEOCODER_XLSX or place the file under the repo or /arcgis/home.\")\n",
    "\n",
    "    try:\n",
    "        geo_df = pd.read_excel(GEOCODER_FILE)\n",
    "        # Peek at columns for sanity\n",
    "        print(\"Geocoder file columns:\", geo_df.columns.tolist())\n",
    "        # Pull extras if present\n",
    "        cols_to_merge = ['designated_area', 'latitude', 'longitude', 'state']\n",
    "        if 'geometry_api' in geo_df.columns:\n",
    "            cols_to_merge.append('geometry_api')\n",
    "        geo_merge_df = geo_df[cols_to_merge].copy()\n",
    "        \n",
    "        # Merge the dataframes, keeping all alerts\n",
    "        merged_df = pd.merge(\n",
    "            alerts_df,\n",
    "            geo_merge_df,\n",
    "            left_on='area_desc',\n",
    "            right_on='designated_area',\n",
    "            how='left'\n",
    "        )\n",
    "        merged_df = merged_df.drop(columns=['designated_area'])\n",
    "        \n",
    "        # Identify zones that still need coordinates\n",
    "        zones_to_fetch = merged_df[merged_df['latitude'].isna()]['zone_id'].unique()\n",
    "        \n",
    "        # Create a cache to store new coordinates\n",
    "        new_coords_cache = {}\n",
    "        \n",
    "        if len(zones_to_fetch) > 0:\n",
    "            logging.info(f\"Found {len(zones_to_fetch)} zones missing from local file. Attempting live API lookup...\")\n",
    "            for zone_id in zones_to_fetch:\n",
    "                try:\n",
    "                    zone_type = 'forecast'\n",
    "                    if 'PRC' in zone_id: zone_type = 'county'\n",
    "                    if 'AMZ' in zone_id: zone_type = 'marine'\n",
    "                    \n",
    "                    zone_url = f\"https://api.weather.gov/zones/{zone_type}/{zone_id}\"\n",
    "                    z_r = requests.get(zone_url, headers=HEADERS, timeout=20)\n",
    "                    z_r.raise_for_status()\n",
    "                    z_data = z_r.json()\n",
    "                    \n",
    "                    geom_data = z_data.get(\"geometry\")\n",
    "                    if geom_data and geom_data.get('coordinates'):\n",
    "                        first_ring = geom_data['coordinates'][0]\n",
    "                        if geom_data['type'] == 'MultiPolygon':\n",
    "                            first_ring = first_ring[0]\n",
    "                        \n",
    "                        if first_ring:\n",
    "                            lon_list = [p[0] for p in first_ring]\n",
    "                            lat_list = [p[1] for p in first_ring]\n",
    "                            lon = sum(lon_list) / len(lon_list)\n",
    "                            lat = sum(lat_list) / len(lat_list)\n",
    "                            state = z_data.get(\"properties\", {}).get(\"state\", \"PR\")\n",
    "                            new_coords_cache[zone_id] = {'latitude': lat, 'longitude': lon, 'state': state}\n",
    "                            # If we want geometry_api from API (as JSON str), add: 'geometry_api': str(geom_data)\n",
    "                            logging.info(f\"✓ Cached API Fallback for '{zone_id}' -> Lat: {lat:.4f}, Lon: {lon:.4f}\")\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"API lookup failed for '{zone_id}': {e}\")\n",
    "\n",
    "        # Apply the cached coordinates vectorized (preserves dtypes, faster)\n",
    "        if new_coords_cache:\n",
    "            missing_mask = merged_df['latitude'].isna()\n",
    "            for col, cache_key in zip(['latitude', 'longitude', 'state'], ['latitude', 'longitude', 'state']):\n",
    "                merged_df.loc[missing_mask, col] = merged_df.loc[missing_mask, 'zone_id'].map(lambda z: new_coords_cache.get(z, {}).get(cache_key))\n",
    "            # If caching geometry_api: zip(['...','geometry_api'], [...'geometry_api']) etc.\n",
    "\n",
    "        logging.info(\"--- GEOCODING SWEEP COMPLETE ---\")\n",
    "        alerts_df = merged_df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(f\"Geocoder file not found at {GEOCODER_FILE}. Geocoding skipped – no coords added!\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Geocoding sweep bombed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()  # Spew the full stack for us to dissect\n",
    "\n",
    "\n",
    "# Display a preview of the full DataFrame\n",
    "if not alerts_df.empty:\n",
    "    alerts_df_sorted = alerts_df.sort_values(by=['event', 'area_desc'], ascending=[False, True])\n",
    "    logging.info(\"DataFrame Preview:\")\n",
    "    display(alerts_df_sorted)\n",
    "else:\n",
    "    logging.info(\"No records to process.\")\n",
    "\n",
    "print(\"\\nCell 5: DataFrame built and geocoded.\")\n",
    "\n",
    "\n",
    "if not USE_ARCGIS:\n",
    "    df_out = alerts_df_sorted if \"alerts_df_sorted\" in locals() else alerts_df\n",
    "    if df_out is None or df_out.empty:\n",
    "        logging.info(\"No records to write locally.\")\n",
    "    else:\n",
    "        df_out.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "        def to_jsonable(val):\n",
    "            if isinstance(val, pd.Timestamp):\n",
    "                return val.isoformat()\n",
    "            try:\n",
    "                if pd.isna(val):\n",
    "                    return None\n",
    "            except Exception:\n",
    "                pass\n",
    "            if hasattr(val, \"item\"):\n",
    "                try:\n",
    "                    return val.item()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return val\n",
    "\n",
    "        features = []\n",
    "        if {\"longitude\", \"latitude\"}.issubset(df_out.columns):\n",
    "            for _, row in df_out.iterrows():\n",
    "                lon = row.get(\"longitude\")\n",
    "                lat = row.get(\"latitude\")\n",
    "                if pd.notna(lon) and pd.notna(lat):\n",
    "                    props = row.drop(labels=[\"longitude\", \"latitude\"]).to_dict()\n",
    "                    props = {k: to_jsonable(v) for k, v in props.items()}\n",
    "                    features.append({\n",
    "                        \"type\": \"Feature\",\n",
    "                        \"geometry\": {\"type\": \"Point\", \"coordinates\": [float(lon), float(lat)]},\n",
    "                        \"properties\": props\n",
    "                    })\n",
    "        geojson = {\"type\": \"FeatureCollection\", \"features\": features}\n",
    "        with open(OUTPUT_GEOJSON, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(geojson, f, ensure_ascii=False, indent=2)\n",
    "        logging.info(\"Local outputs written: %s and %s\", OUTPUT_CSV, OUTPUT_GEOJSON)\n",
    "\n",
    "\n",
    "if USE_ARCGIS:\n",
    "    # Cell 6: Access and Truncate Target Feature Layer\n",
    "    # =============================================================================\n",
    "    try:\n",
    "        logging.info(\"Accessing Feature Layer\")\n",
    "        feature_layer_item = gis.content.get(FEATURE_LAYER_ITEM_ID)\n",
    "        flayer = feature_layer_item.layers[LAYER_INDEX]\n",
    "        logging.info(f\"Target layer: '{flayer.properties.name}'\")\n",
    "\n",
    "        # Truncate the layer\n",
    "        count = flayer.query(return_count_only=True)\n",
    "        logging.info(f\"Existing feature count: {count}\")\n",
    "        if count > 0:\n",
    "            flayer.delete_features(where=\"1=1\")\n",
    "            logging.info(\"Layer successfully cleared.\")\n",
    "        else:\n",
    "            logging.info(\"Layer is already empty.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"FATAL: Could not access or truncate the feature layer. Error: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"\\nCell 6: Feature Layer truncate complete.\")\n",
    "\n",
    "\n",
    "    # Cell 7: Prepare and Push Data to ArcGIS\n",
    "    # =============================================================================\n",
    "    if not alerts_df.empty:\n",
    "        try:\n",
    "            adds = []\n",
    "            # Use the sorted dataframe for processing\n",
    "            df_clean = alerts_df_sorted.copy()\n",
    "\n",
    "            # Convert datetimes to strings for AGOL\n",
    "            df_clean['sent'] = df_clean['sent'].dt.strftime('%Y-%m-%d %H:%M:%S').fillna('')\n",
    "            df_clean['expires'] = df_clean['expires'].dt.strftime('%Y-%m-%d %H:%M:%S').fillna('')\n",
    "\n",
    "            for _, row in df_clean.iterrows():\n",
    "                attrs = row.to_dict()\n",
    "            \n",
    "                # Clean up potential NaN values and internal columns\n",
    "                internal_cols = ['zone_id']\n",
    "                clean_attrs = {k: v for k, v in attrs.items() if k not in internal_cols and pd.notna(v)}\n",
    "\n",
    "                # Explicitly cast numeric types to prevent silent errors\n",
    "                if 'latitude' in clean_attrs:\n",
    "                    clean_attrs['latitude'] = float(clean_attrs['latitude'])\n",
    "                if 'longitude' in clean_attrs:\n",
    "                    clean_attrs['longitude'] = float(clean_attrs['longitude'])\n",
    "\n",
    "                # Create geometry from the now-populated coordinate fields\n",
    "                geom = None\n",
    "                if pd.notna(row.get('longitude')) and pd.notna(row.get('latitude')):\n",
    "                    geom = Point({\n",
    "                        \"x\": row['longitude'],\n",
    "                        \"y\": row['latitude'],\n",
    "                        \"spatialReference\": {\"wkid\": 4326}\n",
    "                    })\n",
    "\n",
    "                adds.append(Feature(geometry=geom, attributes=clean_attrs))\n",
    "\n",
    "            logging.info(f\"Prepared {len(adds)} features for upload.\")\n",
    "\n",
    "            # Push edits to the feature layer\n",
    "            if adds:\n",
    "                result = flayer.edit_features(adds=adds, rollback_on_failure=True)\n",
    "            \n",
    "                # Verify success\n",
    "                add_results = result.get(\"addResults\", [])\n",
    "                success_count = sum(1 for r in add_results if r.get(\"success\"))\n",
    "                if success_count == len(adds):\n",
    "                    logging.info(f\"✔ Successfully added {success_count} features to the layer.\")\n",
    "                else:\n",
    "                    logging.error(\"Some features failed to add. See detailed response:\")\n",
    "                    logging.error(result)\n",
    "            else:\n",
    "                logging.info(\"No features to add.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"FATAL: An error occurred while preparing or pushing edits: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        logging.info(\"DataFrame is empty, nothing to push to ArcGIS.\")\n",
    "\n",
    "    print(\"\\n--- WORKFLOW COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas full tables\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alerts_df_sorted.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(alerts_df_sorted[[\"sent\",\"event\",\"headline\",\"description\",\"instruction\",\"response\",\"severity\",\"urgency\",\"certainty\",\"area_desc\",\"expires\",\"status\",id\tzone_id\",\"latitude\",\"longitude\",\"state\",\"area_desc\"]])\n",
    "display(alerts_df_sorted[[\"zone_id\",\"event\", \"latitude\",\"longitude\",\"state\",\"area_desc\", \"expires\"]])\n",
    "# display(alerts_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "esriNotebookRuntime": {
   "notebookRuntimeName": "ArcGIS Notebook Python 3 Standard",
   "notebookRuntimeVersion": "12.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
