{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to your notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this cell to connect to your GIS and get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from arcgis.gis import GIS\n",
    "# import contextlib, io\n",
    "# with contextlib.redirect_stderr(io.StringIO()):\n",
    "#     gis = GIS(\"home\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now you are ready to start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -*- coding: utf-8 -*-\n",
    "# \"\"\"\n",
    "# Title: USGS Daily Water Data to ArcGIS Feature Layer (Notebook Version)\n",
    "# Description: A script designed for cell-by-cell execution in an ArcGIS Notebook.\n",
    "#              It fetches daily water data from the USGS Water Services API, processes\n",
    "#              it into a pandas DataFrame, and updates a target ArcGIS Feature Layer\n",
    "#              using a robust Truncate and Add workflow with edit_features.\n",
    "# v10 – Final Production Version.\n",
    "# \"\"\"\n",
    "#\n",
    "# Optional: ArcGIS Online Sync\n",
    "# To publish updates to ArcGIS Online, set:\n",
    "#   USE_ARCGIS=1\n",
    "#   USGS_WATER_LAYER_ID (or FEATURE_LAYER_ITEM_ID)\n",
    "# Then re-run the notebook from Cell 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install and Import Libraries\n",
    "# =================================================================================\n",
    "# This cell ensures the necessary libraries are installed in the notebook environment.\n",
    "import sys\n",
    "import subprocess\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Set USE_ARCGIS=1 to enable ArcGIS Online sync; otherwise run locally.\n",
    "USE_ARCGIS = os.environ.get(\"USE_ARCGIS\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
    "\n",
    "print(\"Installing required libraries...\")\n",
    "base_pkgs = [\"pandas\", \"openpyxl\", \"requests\"]\n",
    "if USE_ARCGIS:\n",
    "    base_pkgs.append(\"arcgis\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *base_pkgs])\n",
    "print(\"Installation complete.\")\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "if USE_ARCGIS:\n",
    "    from arcgis.gis import GIS\n",
    "    from arcgis.features import Feature\n",
    "    from arcgis.geometry import Point\n",
    "\n",
    "# Import display for rich DataFrame output in notebooks\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except ImportError:\n",
    "    display = print # Fallback to standard print if IPython is not available\n",
    "\n",
    "# Configure logging to display messages in the notebook output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    stream=sys.stdout\n",
    ")\n",
    "\n",
    "print(\"Cell 1: Libraries installed and imported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show me the version\n",
    "from platform import python_version\n",
    "print(\"Python version:\", python_version())\n",
    "\n",
    "pkgs = [pd, logging, requests, json]\n",
    "if USE_ARCGIS:\n",
    "    import arcgis\n",
    "    pkgs.insert(1, arcgis)\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p.__name__:-<30}v{p.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas full tables\n",
    "# import warnings\n",
    "# warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣  Busca TODAS las Feature Layers de la organización sin importar el autor (*)\n",
    "if USE_ARCGIS:\n",
    "    fl_items = gis.content.search(\n",
    "        query=\"*\",                  # comodín (puede ser \"\" o \"*\")\n",
    "        item_type=\"Feature Layer\",\n",
    "        max_items=1000              # ajusta si tienes más\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame([\n",
    "        {\"Title\": it.title, \"ITEM_ID\": it.itemid, \"Layers\": len(it.layers)}\n",
    "        for it in fl_items\n",
    "    ]).sort_values(\"Title\")\n",
    "\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"ArcGIS disabled; skipping organization search.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install and Import Libraries\n",
    "# =================================================================================\n",
    "# This cell ensures the necessary libraries are installed in the notebook environment.\n",
    "import sys\n",
    "import subprocess\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Set USE_ARCGIS=1 to enable ArcGIS Online sync; otherwise run locally.\n",
    "USE_ARCGIS = os.environ.get(\"USE_ARCGIS\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
    "\n",
    "print(\"Installing required libraries...\")\n",
    "base_pkgs = [\"pandas\", \"openpyxl\", \"requests\"]\n",
    "if USE_ARCGIS:\n",
    "    base_pkgs.append(\"arcgis\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *base_pkgs])\n",
    "print(\"Installation complete.\")\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "if USE_ARCGIS:\n",
    "    from arcgis.gis import GIS\n",
    "    from arcgis.features import Feature\n",
    "    from arcgis.geometry import Point\n",
    "\n",
    "# Import display for rich DataFrame output in notebooks\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except ImportError:\n",
    "    display = print # Fallback to standard print if IPython is not available\n",
    "\n",
    "# Configure logging to display messages in the notebook output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    stream=sys.stdout\n",
    ")\n",
    "\n",
    "print(\"Cell 1: Libraries installed and imported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "# =================================================================================\n",
    "# --- USER-DEFINED VARIABLES ---\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set USE_ARCGIS=1 to enable ArcGIS Online sync; otherwise run locally.\n",
    "USE_ARCGIS = os.environ.get(\"USE_ARCGIS\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
    "\n",
    "def resolve_file(filename, env_var=None, search_roots=None):\n",
    "    if env_var:\n",
    "        env_val = os.environ.get(env_var)\n",
    "        if env_val:\n",
    "            return env_val\n",
    "    roots = search_roots or [Path.cwd(), Path.cwd().parent, Path.home()]\n",
    "    arcgis_home = Path(\"/arcgis/home\")\n",
    "    if arcgis_home.exists():\n",
    "        roots.append(arcgis_home)\n",
    "    for root in roots:\n",
    "        if root.exists():\n",
    "            match = next(root.rglob(filename), None)\n",
    "            if match:\n",
    "                return str(match)\n",
    "    raise FileNotFoundError(\"Set the required env var or place the file under the repo or /arcgis/home.\")\n",
    "\n",
    "# The Item ID of the target Hosted Feature Layer in ArcGIS.\n",
    "FEATURE_LAYER_ITEM_ID = os.environ.get(\"USGS_WATER_LAYER_ID\") or os.environ.get(\"FEATURE_LAYER_ITEM_ID\")\n",
    "\n",
    "# The index of the layer within the Feature Layer Collection to be updated.\n",
    "LAYER_INDEX = 0\n",
    "\n",
    "if USE_ARCGIS and not FEATURE_LAYER_ITEM_ID:\n",
    "    raise ValueError(\"Set USGS_WATER_LAYER_ID (or FEATURE_LAYER_ITEM_ID) in the environment.\")\n",
    "\n",
    "# Resolve local configuration files without hardcoding paths.\n",
    "DATA_SOURCES_FILE = resolve_file(\"PR Alert Data Sources.xlsx\", env_var=\"PR_ALERT_XLSX\")\n",
    "PARAMETER_CODE_FILE = resolve_file(\"parameter_code_codetable.xlsx\", env_var=\"PARAMETER_CODE_XLSX\")\n",
    "SCHEMADEF_FILE = resolve_file(\"USGS_Daily_Watet_schemadefinitions.xlsx\", env_var=\"USGS_SCHEMA_XLSX\")\n",
    "STATISTIC_ID_FILE = resolve_file(\"statistic_id_codetable.xlsx\", env_var=\"STATISTIC_ID_XLSX\")\n",
    "\n",
    "# Local outputs (for non-ArcGIS runs)\n",
    "OUTPUT_DIR = Path(os.environ.get(\"OUTPUT_DIR\", \"outputs\"))\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_CSV = OUTPUT_DIR / \"usgs_water.csv\"\n",
    "OUTPUT_GEOJSON = OUTPUT_DIR / \"usgs_water.geojson\"\n",
    "\n",
    "# The name of the API source to process from the configuration file.\n",
    "SOURCE_NAME = \"USGS - Daily Water Data (New API)\"\n",
    "PAGE_LIMIT = 250\n",
    "\n",
    "print(\"Cell 2: Configuration variables set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Connect to your ArcGIS Organization (optional)\n",
    "# =================================================================================\n",
    "if USE_ARCGIS:\n",
    "    try:\n",
    "        logging.info(\"Connecting to ArcGIS environment...\")\n",
    "        # The \"home\" parameter uses the credentials of the user running the notebook\n",
    "        gis = GIS(\"home\")\n",
    "        logging.info(\"Successfully connected to %s.\", gis.properties.portalHostname)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"FATAL: Failed to connect to ArcGIS. Error: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"Cell 3: ArcGIS connection established.\")\n",
    "else:\n",
    "    gis = None\n",
    "    logging.info(\"ArcGIS disabled; running locally only.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Fetch Today's USGS Data (Loyal to Reference Script)\n",
    "# =================================================================================\n",
    "logging.info(\"Fetching data from source configuration...\")\n",
    "try:\n",
    "    row = pd.read_excel(DATA_SOURCES_FILE).query(\"Source_Name == @SOURCE_NAME\").squeeze()\n",
    "    api_url = row[\"URL_Endpoint\"]\n",
    "    \n",
    "    # This logic now faithfully replicates the user's working script\n",
    "    api_params = {}\n",
    "    params_str = row.get(\"Parameters_or_Selectors\")\n",
    "    if pd.notna(params_str):\n",
    "        try:\n",
    "            api_params = json.loads(params_str)\n",
    "            logging.info(\"Successfully parsed parameters from Excel.\")\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            logging.error(f\"Could not parse 'Parameters_or_Selectors' for {SOURCE_NAME}. Treating as empty.\")\n",
    "            api_params = {}\n",
    "\n",
    "    api_params.setdefault(\"limit\", PAGE_LIMIT)\n",
    "    logging.info(f\"Page limit set to: {api_params['limit']}\")\n",
    "\n",
    "    if pd.notna(row.get(\"Bounding_Box\")):\n",
    "        api_params[\"bbox\"] = str(row[\"Bounding_Box\"])\n",
    "        logging.info(f\"Using Bounding Box: {api_params['bbox']}\")\n",
    "\n",
    "    if pd.notna(row.get(\"API_Key\")):\n",
    "        api_params[\"api_key\"] = str(row[\"API_Key\"])\n",
    "        logging.info(\"Using API Key.\")\n",
    "\n",
    "    items, nxt, page = [], api_url, 0\n",
    "    while nxt:\n",
    "        page += 1\n",
    "        logging.info(\"Fetching page %d…\", page)\n",
    "        # The key is to only send the full parameter set on the FIRST request.\n",
    "        # Subsequent requests use the 'next' URL which already has the parameters baked in.\n",
    "        r = requests.get(nxt, params=api_params if page == 1 else {}, timeout=120)\n",
    "        r.raise_for_status()\n",
    "        j = r.json()\n",
    "        items.extend(j.get(\"features\", []))\n",
    "        nxt = next((l[\"href\"] for l in j.get(\"links\", []) if l.get(\"rel\") == \"next\"), None)\n",
    "    \n",
    "    logging.info(\"Total fetched: %d\", len(items))\n",
    "    if not items:\n",
    "        sys.exit(\"Execution stopped: No data fetched from the API.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"FATAL: An error occurred during data fetching: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"Cell 4: Data fetching complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.2: Load Parameter Codes definitions\n",
    "# pandas full tables\n",
    "# import warnings\n",
    "# warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "parameter_code_df = pd.read_excel(PARAMETER_CODE_FILE)\n",
    "\n",
    "parameter_code_df[\"parameter_code\"] = (\n",
    "    parameter_code_df[\"parameter_code\"]\n",
    "    .astype(\"Int64\")     # handles NaNs safely\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    "    .astype(str)\n",
    "    .str.zfill(5)        # ensures 5-digit zero-padding\n",
    "    .astype(\"object\")    # stored as native Python string\n",
    ")\n",
    "\n",
    "\n",
    "# sanity-check\n",
    "print(parameter_code_df.dtypes)\n",
    "# limit the number of culumns to: parameter_code\tGroup Name\tParameter Name/Description\n",
    "parameter_code_df=parameter_code_df[[\"parameter_code\",\"Group Name\",\"Parameter Name/Description\"]]\n",
    "display(parameter_code_df)\n",
    "\n",
    "\n",
    "# pandas full tables\n",
    "# import warnings\n",
    "# warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.1: Load Schema Definitions (To use later)\n",
    "# pandas full tables\n",
    "# import warnings\n",
    "# warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "schemadef=pd.read_excel(SCHEMADEF_FILE)\n",
    "display(schemadef)\n",
    "\n",
    "# pandas full tables\n",
    "# import warnings\n",
    "# warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.3: Load statistic_id definitions\n",
    "# pandas full tables\n",
    "# import warnings\n",
    "# warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "statistic_id_df = pd.read_excel(STATISTIC_ID_FILE)\n",
    "\n",
    "statistic_id_df[\"statistic_id\"] = (\n",
    "    statistic_id_df[\"statistic_id\"]\n",
    "    .astype(\"Int64\")  # handles NaNs, trims decimals\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    "    .astype(str)\n",
    "    .str.zfill(5)     # pads with leading zeros to 5 digits\n",
    "    .astype(\"object\") # ensures native Python string type\n",
    ")\n",
    "\n",
    "# sanity-check\n",
    "print(statistic_id_df.dtypes)\n",
    "display(statistic_id_df)\n",
    "\n",
    "# pandas full tables\n",
    "# import warnings\n",
    "# warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Build DataFrame\n",
    "# =================================================================================\n",
    "logging.info(\"Building DataFrame from fetched data...\")\n",
    "try:\n",
    "    rows = []\n",
    "    for f in items:\n",
    "        p, g = f[\"properties\"], f[\"geometry\"]\n",
    "        lon, lat = (g.get(\"coordinates\") or [None, None])[:2]\n",
    "\n",
    "        rows.append({\n",
    "            \"geom_type\": g.get(\"type\", \"Point\"),\n",
    "            \"longitude\": float(lon) if lon is not None else None,\n",
    "            \"latitude\": float(lat) if lat is not None else None,\n",
    "            \"time_series_id\": p.get(\"timeseries_id\"),\n",
    "            \"monitoring_location_id\": p.get(\"monitoring_location_id\"),\n",
    "            \"value\": p.get(\"value\"),\n",
    "            \"approval_status\": p.get(\"approval_status\"),\n",
    "            \"last_modified\": p.get(\"last_modified\"),\n",
    "            \"parameter_code\": p.get(\"parameter_code\"),\n",
    "            \"statistic_id\": p.get(\"statistic_id\"),\n",
    "            \"unit_of_measure\": p.get(\"unit_of_measure\"),\n",
    "            \"qualifier\": p.get(\"qualifier\"),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    logging.info(\"Rows in DataFrame: %d\", len(df))\n",
    "\n",
    "    # merge parameter_code\n",
    "    df=df.merge(parameter_code_df, on=\"parameter_code\", how=\"left\")\n",
    "\n",
    "    # merge statistic_id\n",
    "    df=df.merge(statistic_id_df, on=\"statistic_id\", how=\"left\")\n",
    "    \n",
    "    # --- Display a sample of the DataFrame ---\n",
    "    sample_size = 5\n",
    "    if len(df) > (sample_size * 2):\n",
    "        logging.info(f\"Displaying first and last {sample_size} rows as a sample.\")\n",
    "        display(pd.concat([df.head(sample_size), df.tail(sample_size)]))\n",
    "    else:\n",
    "        display(df)\n",
    "\n",
    "    if not USE_ARCGIS:\n",
    "        if df.empty:\n",
    "            logging.info(\"No records to write locally.\")\n",
    "        else:\n",
    "            df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "            def to_jsonable(val):\n",
    "                if isinstance(val, pd.Timestamp):\n",
    "                    return val.isoformat()\n",
    "                try:\n",
    "                    if pd.isna(val):\n",
    "                        return None\n",
    "                except Exception:\n",
    "                    pass\n",
    "                if hasattr(val, \"item\"):\n",
    "                    try:\n",
    "                        return val.item()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                return val\n",
    "\n",
    "            features = []\n",
    "            if {\"longitude\", \"latitude\"}.issubset(df.columns):\n",
    "                for _, row in df.iterrows():\n",
    "                    lon = row.get(\"longitude\")\n",
    "                    lat = row.get(\"latitude\")\n",
    "                    if pd.notna(lon) and pd.notna(lat):\n",
    "                        props = row.drop(labels=[\"longitude\", \"latitude\"]).to_dict()\n",
    "                        props = {k: to_jsonable(v) for k, v in props.items()}\n",
    "                        features.append({\n",
    "                            \"type\": \"Feature\",\n",
    "                            \"geometry\": {\"type\": \"Point\", \"coordinates\": [float(lon), float(lat)]},\n",
    "                            \"properties\": props\n",
    "                        })\n",
    "            geojson = {\"type\": \"FeatureCollection\", \"features\": features}\n",
    "            with open(OUTPUT_GEOJSON, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(geojson, f, ensure_ascii=False, indent=2)\n",
    "            logging.info(\"Local outputs written: %s and %s\", OUTPUT_CSV, OUTPUT_GEOJSON)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"FATAL: An error occurred while building the DataFrame: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"Cell 5: DataFrame built successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity-check\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Group Name\"].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Parameter Name/Description\"].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Stat Nm\"].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Stat Ds\"].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"qualifier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"qualifier\"].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"qualifier\"].head(10).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifier_norm = df[\"qualifier\"].apply(\n",
    "    lambda x: tuple(x) if isinstance(x, list) else x   # list → tuple\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifier_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣  Build a Boolean mask: True only for list-type entries\n",
    "mask_lists = df[\"qualifier\"].apply(lambda x: isinstance(x, list))\n",
    "\n",
    "# 2️⃣  Slice the dataframe with that mask\n",
    "df_lists = df[mask_lists]\n",
    "\n",
    "# 3️⃣  Take a look\n",
    "print(f\"{df_lists.shape[0]} rows with list qualifiers:\")\n",
    "display(df_lists.head())        # Jupyter friendly; or .to_markdown(), .to_string(), etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_values = df.loc[mask_lists, \"qualifier\"].tolist()\n",
    "for i, val in enumerate(list_values, start=1):\n",
    "    print(f\"{i:>2}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coerce the lists into simple strings so the column is hash-friendly\n",
    "def squash_list(x, sep=\" - \"):\n",
    "    \"\"\"\n",
    "    • If x is a list  ➜  join its items with sep      ['A','B'] → \"A - B\"\n",
    "    • Otherwise      ➜  leave it as-is (None, str…)\n",
    "    \"\"\"\n",
    "    return sep.join(x) if isinstance(x, list) else x\n",
    "\n",
    "df[\"qualifier\"] = df[\"qualifier\"].apply(squash_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"qualifier\"].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Access Target Layer\n",
    "# =================================================================================\n",
    "if not USE_ARCGIS:\n",
    "    print(\"ArcGIS disabled; skipping Cell 6.\")\n",
    "else:\n",
    "    try:\n",
    "        flayer = gis.content.get(FEATURE_LAYER_ITEM_ID).layers[LAYER_INDEX]\n",
    "        logging.info(\"Target layer: %s\", flayer.properties.name)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"FATAL: Could not access target feature layer: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"Cell 6: Target layer accessed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Truncate Layer\n",
    "# =================================================================================\n",
    "if not USE_ARCGIS:\n",
    "    print(\"ArcGIS disabled; skipping Cell 7.\")\n",
    "else:\n",
    "    try:\n",
    "        count = flayer.query(return_count_only=True)\n",
    "        logging.info(\"Existing feature count: %d\", count)\n",
    "        if count:\n",
    "            flayer.delete_features(where=\"1=1\")\n",
    "            logging.info(\"Layer cleared.\")\n",
    "        else:\n",
    "            logging.info(\"Layer already empty.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"FATAL: An error occurred during truncation: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"Cell 7: Truncate operation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 8: Prepare and Push Adds\n",
    "# =================================================================================\n",
    "if not USE_ARCGIS:\n",
    "    print(\"ArcGIS disabled; skipping Cell 8.\")\n",
    "else:\n",
    "    try:\n",
    "        adds = []\n",
    "        has_geom = bool(getattr(flayer.properties, \"geometryType\", \"\"))\n",
    "    \n",
    "        # Create a copy to avoid SettingWithCopyWarning\n",
    "        df_clean = df.copy()\n",
    "\n",
    "        # --- NEW: Rename DataFrame columns to exactly match Feature Layer field names ---\n",
    "        column_rename_map = {\n",
    "            \"Group Name\": \"Group_Name\",\n",
    "            \"Parameter Name/Description\": \"Parameter_Name_Description\",\n",
    "            \"Stat Nm\": \"Stat_Nm\",\n",
    "            \"Stat Ds\": \"Stat_Ds\"\n",
    "        }\n",
    "        df_clean.rename(columns=column_rename_map, inplace=True)\n",
    "        logging.info(\"Renamed DataFrame columns to match feature layer schema.\")\n",
    "        # -----------------------------------------------------------------------------\n",
    "    \n",
    "        for _, row in df_clean.iterrows():\n",
    "            # --- Create a dictionary of all attributes from the row ---\n",
    "            attrs = row.to_dict()\n",
    "        \n",
    "            # --- Date Handling (DO NOT CHANGE) ---\n",
    "            ts_object = pd.to_datetime(attrs.get('last_modified'), utc=True, errors='coerce')\n",
    "\n",
    "            if pd.notna(ts_object):\n",
    "                attrs['last_modified'] = ts_object.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                attrs['time'] = ts_object.strftime('%Y-%m-%d')\n",
    "            else:\n",
    "                attrs['last_modified'] = None\n",
    "                attrs['time'] = None\n",
    "        \n",
    "            # --- Explicit Type Casting for Problem Fields ---\n",
    "            attrs['longitude'] = float(attrs['longitude']) if pd.notna(attrs.get('longitude')) else None\n",
    "            attrs['latitude'] = float(attrs['latitude']) if pd.notna(attrs.get('latitude')) else None\n",
    "            attrs['value'] = float(attrs['value']) if pd.notna(attrs.get('value')) else None\n",
    "            attrs['parameter_code'] = int(attrs['parameter_code']) if pd.notna(attrs.get('parameter_code')) else None\n",
    "            attrs['statistic_id'] = int(attrs['statistic_id']) if pd.notna(attrs.get('statistic_id')) else None\n",
    "        \n",
    "            # Cast all text fields to string to be safe, now using the RENAMED column names\n",
    "            attrs['time_series_id'] = str(attrs['time_series_id']) if pd.notna(attrs.get('time_series_id')) else None\n",
    "            attrs['qualifier'] = str(attrs['qualifier']) if pd.notna(attrs.get('qualifier')) else None\n",
    "            attrs['Group_Name'] = str(attrs.get('Group_Name')) if pd.notna(attrs.get('Group_Name')) else None\n",
    "            attrs['Parameter_Name_Description'] = str(attrs.get('Parameter_Name_Description')) if pd.notna(attrs.get('Parameter_Name_Description')) else None\n",
    "            attrs['Stat_Nm'] = str(attrs.get('Stat_Nm')) if pd.notna(attrs.get('Stat_Nm')) else None\n",
    "            attrs['Stat_Ds'] = str(attrs.get('Stat_Ds')) if pd.notna(attrs.get('Stat_Ds')) else None\n",
    "        \n",
    "            # Final cleanup of any remaining None/NaN values\n",
    "            clean_attrs = {k: v for k, v in attrs.items() if v is not None}\n",
    "\n",
    "            # --- Geometry Creation ---\n",
    "            if has_geom and pd.notna(row[\"longitude\"]) and pd.notna(row[\"latitude\"]):\n",
    "                geom = Point({\"x\": row[\"longitude\"], \"y\": row[\"latitude\"], \"spatialReference\": {\"wkid\": 4326}})\n",
    "                adds.append(Feature(geometry=geom, attributes=clean_attrs))\n",
    "            else:\n",
    "                adds.append(Feature(attributes=clean_attrs))\n",
    "\n",
    "        # --- Final Pre-flight Check (Commented out for production runs) ---\n",
    "        # if adds:\n",
    "        #     print(\"\\n\" + \"=\"*25 + \" DEBUG: FIRST FEATURE OBJECT TO BE SENT \" + \"=\"*25)\n",
    "        #     # Correctly serialize the Feature object's dictionary for printing\n",
    "        #     print(json.dumps(adds[0].as_dict, indent=2))\n",
    "        #     print(\"=\"*75 + \"\\n\")\n",
    "\n",
    "        logging.info(\"Prepared %d features for add.\", len(adds))\n",
    "\n",
    "        # --- Pushing the Edits ---\n",
    "        if adds:\n",
    "            result = flayer.edit_features(adds=adds, rollback_on_failure=True)\n",
    "            # The detailed response is now commented out for cleaner production logs.\n",
    "            # logging.info(\"edit_features response:\\n%s\", json.dumps(result, indent=2))\n",
    "\n",
    "            ok = all(r.get(\"success\") for r in result.get(\"addResults\", []))\n",
    "            if not ok:\n",
    "                raise RuntimeError(\"Some features failed to add – see response above.\")\n",
    "\n",
    "            logging.info(\"✔ All features added (%d)\", len(adds))\n",
    "        else:\n",
    "            logging.info(\"No features to add.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"FATAL: An error occurred while preparing or pushing edits: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"--- WORKFLOW COMPLETE ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "esriNotebookRuntime": {
   "notebookRuntimeName": "ArcGIS Notebook Python 3 Standard",
   "notebookRuntimeVersion": "12.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
