{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# NOAA Sensors (Puerto Rico) - Resilient Water Level Hydrograph\n\nGitHub: https://github.com/yagaC64/Spring2026DAEN\n\nLicense: https://github.com/yagaC64/Spring2026DAEN/blob/main/LICENSE\n\nThis notebook is the non-AGOL, data-science/student version of the Puerto Rico NOAA workflow.\n\nWhat this notebook does:\n- builds the station list algorithmically from NOAA MDAPI (no hardwired dependency required),\n- applies resilient station resolution and compatibility filtering,\n- fetches observed water-level time series with robust request/error handling,\n- exports one comprehensive standalone HTML report.\n\nOutput policy:\n- exactly one HTML file is produced (`OUTPUT_HTML`).\n- CSV exports are also written for analysis/reproducibility.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Install and import libraries\n# =================================================================================\nimport importlib.util\nimport logging\nimport os\nimport subprocess\nimport sys\nimport time\nfrom datetime import datetime, timedelta, timezone\nfrom pathlib import Path\n\n\ndef ensure_packages(packages):\n    missing = [pkg for pkg in packages if importlib.util.find_spec(pkg) is None]\n    if missing:\n        print(f\"Installing missing packages: {missing}\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", *missing])\n\n\nensure_packages([\"pandas\", \"requests\", \"plotly\"])\n\nimport pandas as pd\nimport requests\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ntry:\n    from IPython.display import display\nexcept ImportError:\n    display = print\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\nlogger = logging.getLogger(\"noaa-pr-hydrograph\")\n\nprint(\"Cell 1 complete.\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Configuration\n# =================================================================================\nNOAA_STATE = \"PR\"\nNOAA_PRODUCT = os.environ.get(\"NOAA_PRODUCT\", \"water_level\")\nNOAA_DATUM = os.environ.get(\"NOAA_DATUM\", \"MLLW\")\nNOAA_TIME_ZONE = os.environ.get(\"NOAA_TIME_ZONE\", \"gmt\")\nNOAA_UNITS = os.environ.get(\"NOAA_UNITS\", \"metric\")\n\nLOOKBACK_HOURS = int(os.environ.get(\"LOOKBACK_HOURS\", str(int(os.environ.get(\"LOOKBACK_DAYS\", \"7\")) * 24)))\n\n# Station selection strategy (algorithmic first)\nRAW_STATION_IDS = os.environ.get(\"NOAA_STATION_IDS\", \"\").strip()\nEXCLUDE_STATION_IDS = os.environ.get(\"NOAA_EXCLUDE_STATION_IDS\", \"\").strip()\nMAX_ACTIVE_STATIONS = int(os.environ.get(\"MAX_ACTIVE_STATIONS\", \"100\"))\nREQUIRE_TIDAL_FOR_DATUM = os.environ.get(\"REQUIRE_TIDAL_FOR_DATUM\", \"true\").lower() in (\"1\", \"true\", \"yes\")\n\n# Catalog safety controls\nCATALOG_TIMEOUT_SECONDS = int(os.environ.get(\"CATALOG_TIMEOUT_SECONDS\", \"90\"))\nCATALOG_MAX_BYTES = int(os.environ.get(\"CATALOG_MAX_BYTES\", \"5000000\"))\nCATALOG_MAX_ROWS = int(os.environ.get(\"CATALOG_MAX_ROWS\", \"5000\"))\n\n# Local outputs (single HTML + CSV files)\nOUTPUT_DIR = Path(os.environ.get(\"OUTPUT_DIR\", \"outputs/noaa_pr\"))\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\nOUTPUT_HTML = OUTPUT_DIR / \"noaa_pr_waterlevel_comprehensive.html\"\nOUTPUT_CSV = OUTPUT_DIR / \"noaa_pr_water_levels_timeseries.csv\"\nOUTPUT_STATION_CSV = OUTPUT_DIR / \"noaa_pr_station_summary.csv\"\n\n# Puerto Rico geographic guardrails\nPR_BBOX = {\n    \"min_lon\": -68.5,\n    \"max_lon\": -65.0,\n    \"min_lat\": 17.5,\n    \"max_lat\": 18.9,\n}\n\n# Minimal coordinate fallbacks for continuity if metadata gaps appear\nSTATION_COORDS_FALLBACK = {\n    \"9755371\": (18.4655, -66.1061),\n    \"9759110\": (17.9733, -67.0469),\n    \"9759938\": (17.9691, -67.0464),\n}\n\nRUN_UTC = datetime.now(timezone.utc)\nBEGIN_UTC = RUN_UTC - timedelta(hours=LOOKBACK_HOURS)\nBEGIN_DATE = BEGIN_UTC.strftime(\"%Y%m%d %H:%M\")\nEND_DATE = RUN_UTC.strftime(\"%Y%m%d %H:%M\")\n\nMDAPI_BASE = \"https://api.tidesandcurrents.noaa.gov/mdapi/prod/webapi\"\nDATAGETTER_URL = \"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\n\nprint(\"Configuration loaded:\")\nprint(f\"  NOAA state: {NOAA_STATE}\")\nprint(f\"  Product/datum/units: {NOAA_PRODUCT} / {NOAA_DATUM} / {NOAA_UNITS}\")\nprint(f\"  Lookback hours: {LOOKBACK_HOURS}\")\nprint(f\"  Manual station IDs override: {RAW_STATION_IDS or '[none: use live catalog]'}\")\nprint(f\"  Excluded station IDs: {EXCLUDE_STATION_IDS or '[none]'}\")\nprint(f\"  Max active stations: {MAX_ACTIVE_STATIONS}\")\nprint(f\"  Require tidal stations for datum compatibility: {REQUIRE_TIDAL_FOR_DATUM}\")\nprint(f\"  Catalog timeout/bytes/rows: {CATALOG_TIMEOUT_SECONDS}s / {CATALOG_MAX_BYTES} / {CATALOG_MAX_ROWS}\")\nprint(f\"  Output directory: {OUTPUT_DIR}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Helper functions\n# =================================================================================\ndef api_get_json(url, params=None, timeout=60, retries=3, backoff_seconds=2, max_bytes=None):\n    last_err = None\n    for attempt in range(1, retries + 1):\n        try:\n            response = requests.get(url, params=params, timeout=timeout)\n\n            if max_bytes is not None:\n                payload_size = len(response.content or b\"\")\n                if payload_size > max_bytes:\n                    raise RuntimeError(\n                        f\"Payload size {payload_size} bytes exceeded limit {max_bytes} bytes for {url}\"\n                    )\n\n            if response.status_code >= 400:\n                message = \"\"\n                try:\n                    err_payload = response.json()\n                    message = (err_payload.get(\"error\") or {}).get(\"message\", \"\")\n                except Exception:\n                    message = (response.text or \"\").strip()[:300]\n\n                # 4xx other than timeout/rate-limit are usually non-transient (do not retry).\n                if 400 <= response.status_code < 500 and response.status_code not in (408, 429):\n                    raise RuntimeError(\n                        f\"Non-retryable NOAA HTTP {response.status_code}: {message or 'client error'}\"\n                    )\n\n                response.raise_for_status()\n\n            return response.json()\n\n        except Exception as exc:\n            last_err = exc\n            if \"Non-retryable NOAA HTTP\" in str(exc):\n                break\n\n            if attempt < retries:\n                sleep_for = backoff_seconds ** attempt\n                logger.warning(\n                    \"Request failed (attempt %s/%s). Retrying in %ss. %s\",\n                    attempt,\n                    retries,\n                    sleep_for,\n                    exc,\n                )\n                time.sleep(sleep_for)\n\n    raise RuntimeError(f\"NOAA request failed after {retries} attempts: {last_err}\")\n\n\ndef parse_station_csv(value):\n    return [sid.strip() for sid in str(value).split(\",\") if sid and sid.strip()]\n\n\ndef in_pr_bbox(lat, lon, bbox=PR_BBOX):\n    return bbox[\"min_lon\"] <= lon <= bbox[\"max_lon\"] and bbox[\"min_lat\"] <= lat <= bbox[\"max_lat\"]\n\n\ndef valid_lat_lon(lat, lon):\n    if pd.isna(lat) or pd.isna(lon):\n        return False\n    lat = float(lat)\n    lon = float(lon)\n    if abs(lat) < 1e-9 and abs(lon) < 1e-9:\n        return False\n    return -90 <= lat <= 90 and -180 <= lon <= 180\n\n\ndef split_primary_flag(flag_value):\n    if flag_value is None or (isinstance(flag_value, float) and pd.isna(flag_value)):\n        return 0\n    text = str(flag_value)\n    try:\n        return int(text.split(\",\")[0])\n    except Exception:\n        return 0\n\n\ndef get_pr_station_catalog(state=\"PR\", timeout_seconds=90, max_bytes=5000000, max_rows=5000):\n    payload = api_get_json(\n        f\"{MDAPI_BASE}/stations.json\",\n        params={\"state\": state},\n        timeout=timeout_seconds,\n        max_bytes=max_bytes,\n    )\n    rows = payload.get(\"stations\", [])\n    if not rows:\n        raise RuntimeError(\"No stations returned by NOAA MDAPI for the configured state.\")\n\n    if max_rows > 0 and len(rows) > max_rows:\n        raise RuntimeError(\n            f\"Catalog row count {len(rows)} exceeded CATALOG_MAX_ROWS={max_rows}. Increase threshold if expected.\"\n        )\n\n    df = pd.DataFrame(rows)\n    if \"id\" in df.columns:\n        df[\"id\"] = df[\"id\"].astype(str).str.strip()\n\n    for c in [\"lat\", \"lng\"]:\n        if c in df.columns:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n\n    if {\"lat\", \"lng\"}.issubset(df.columns):\n        df = df[\n            df.apply(\n                lambda r: in_pr_bbox(float(r[\"lat\"]), float(r[\"lng\"])) if pd.notna(r[\"lat\"]) and pd.notna(r[\"lng\"]) else False,\n                axis=1,\n            )\n        ].copy()\n\n    keep = [c for c in [\"id\", \"name\", \"state\", \"lat\", \"lng\", \"shefcode\", \"tidal\"] if c in df.columns]\n    return df[keep].drop_duplicates(subset=[\"id\"]).sort_values(\"id\").reset_index(drop=True)\n\n\ndef resolve_station_ids(catalog_df, raw_station_ids=\"\", exclude_station_ids=\"\", max_active_stations=100):\n    catalog_ids = sorted(catalog_df[\"id\"].astype(str).tolist())\n    catalog_id_set = set(catalog_ids)\n\n    requested_ids = parse_station_csv(raw_station_ids)\n    excluded_ids = set(parse_station_csv(exclude_station_ids))\n\n    invalid_requested = [sid for sid in requested_ids if sid not in catalog_id_set]\n\n    if requested_ids:\n        selected = [sid for sid in requested_ids if sid in catalog_id_set]\n        selection_mode = \"manual intersect live catalog\"\n        if not selected:\n            logger.warning(\"No manual station IDs matched live catalog; falling back to full live catalog.\")\n            selected = catalog_ids\n            selection_mode = \"auto fallback (full live catalog)\"\n    else:\n        selected = catalog_ids\n        selection_mode = \"auto (full live catalog)\"\n\n    if excluded_ids:\n        selected = [sid for sid in selected if sid not in excluded_ids]\n\n    if max_active_stations > 0 and len(selected) > max_active_stations:\n        logger.warning(\n            \"Selected station count %s exceeds MAX_ACTIVE_STATIONS=%s; truncating list.\",\n            len(selected),\n            max_active_stations,\n        )\n        selected = selected[:max_active_stations]\n\n    return selected, invalid_requested, sorted(excluded_ids), selection_mode\n\n\ndef get_station_metadata(station_id):\n    payload = api_get_json(\n        f\"{MDAPI_BASE}/stations/{station_id}.json\",\n        params={\"expand\": \"floodlevels,details,sensors\"},\n        timeout=90,\n        max_bytes=CATALOG_MAX_BYTES,\n    )\n    stations = payload.get(\"stations\", [])\n    if not stations:\n        raise RuntimeError(f\"NOAA MDAPI did not return metadata for station {station_id}.\")\n    return stations[0]\n\n\ndef fetch_station_observations(station_id, begin_date, end_date):\n    params = {\n        \"product\": NOAA_PRODUCT,\n        \"application\": \"GMU_DAEN_PR\",\n        \"begin_date\": begin_date,\n        \"end_date\": end_date,\n        \"datum\": NOAA_DATUM,\n        \"station\": station_id,\n        \"time_zone\": NOAA_TIME_ZONE,\n        \"units\": NOAA_UNITS,\n        \"format\": \"json\",\n    }\n    payload = api_get_json(DATAGETTER_URL, params=params, timeout=120, max_bytes=CATALOG_MAX_BYTES)\n\n    if \"error\" in payload:\n        message = (payload[\"error\"] or {}).get(\"message\", \"\")\n        if \"No data was found\" in message:\n            return pd.DataFrame(columns=[\"t\", \"v\", \"s\", \"f\", \"q\"])\n        raise RuntimeError(f\"NOAA API error for station {station_id}: {payload['error']}\")\n\n    rows = payload.get(\"data\", [])\n    if not rows:\n        return pd.DataFrame(columns=[\"t\", \"v\", \"s\", \"f\", \"q\"])\n\n    df = pd.DataFrame(rows)\n    for c in [\"v\", \"s\"]:\n        if c in df.columns:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n\n    return df\n\n\nprint(\"Helper functions ready.\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: DataFrame display options\n# =================================================================================\npd.set_option(\"mode.chained_assignment\", None)\npd.set_option(\"display.max_rows\", 50)\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.width\", None)\npd.set_option(\"display.max_colwidth\", None)\n\nprint(\"Cell 4 complete.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## NOAA Field Definitions (`time_utc`, `water_level`, `sigma`, `flags`, `quality`)\n\nField meanings used in this notebook:\n- `time_utc`: observation timestamp in UTC.\n- `water_level`: observed water level (`NOAA_UNITS`) based on selected datum (`NOAA_DATUM`).\n- `sigma`: NOAA sigma value for the measurement.\n- `flags`: comma-separated QA/QC flags from NOAA.\n- `quality`: quality code (`p` preliminary, `v` verified).\n\nReferences:\n- CO-OPS Data API: [https://api.tidesandcurrents.noaa.gov/api/prod/](https://api.tidesandcurrents.noaa.gov/api/prod/)\n- CO-OPS response help: [https://api.tidesandcurrents.noaa.gov/api/prod/responseHelp.html](https://api.tidesandcurrents.noaa.gov/api/prod/responseHelp.html)\n- CO-OPS Metadata API: [https://api.tidesandcurrents.noaa.gov/mdapi/prod/](https://api.tidesandcurrents.noaa.gov/mdapi/prod/)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Build station list from live catalog and fetch observations\n# =================================================================================\ncatalog_pull_utc = datetime.now(timezone.utc)\ncatalog_df = get_pr_station_catalog(\n    state=NOAA_STATE,\n    timeout_seconds=CATALOG_TIMEOUT_SECONDS,\n    max_bytes=CATALOG_MAX_BYTES,\n    max_rows=CATALOG_MAX_ROWS,\n)\n\nlogger.info(\"Catalog pull UTC: %s\", catalog_pull_utc.strftime(\"%Y-%m-%d %H:%M:%S %Z\"))\nlogger.info(\"Puerto Rico station catalog rows after PR filter: %s\", len(catalog_df))\n\nif REQUIRE_TIDAL_FOR_DATUM and NOAA_DATUM.upper() != \"STND\" and \"tidal\" in catalog_df.columns:\n    pre_filter_count = len(catalog_df)\n    catalog_df[\"tidal\"] = catalog_df[\"tidal\"].fillna(False).astype(bool)\n    catalog_df = catalog_df[catalog_df[\"tidal\"]].copy()\n    logger.info(\n        \"Datum compatibility filter (tidal only for datum=%s): %s -> %s stations\",\n        NOAA_DATUM,\n        pre_filter_count,\n        len(catalog_df),\n    )\n\ndisplay(catalog_df.head(30))\n\nSTATION_IDS, invalid_requested_ids, excluded_ids, station_selection_mode = resolve_station_ids(\n    catalog_df,\n    raw_station_ids=RAW_STATION_IDS,\n    exclude_station_ids=EXCLUDE_STATION_IDS,\n    max_active_stations=MAX_ACTIVE_STATIONS,\n)\n\nprint(\"Station selection summary:\")\nprint(f\"  Selection mode: {station_selection_mode}\")\nprint(f\"  Selected station count: {len(STATION_IDS)}\")\nprint(f\"  Selected station IDs: {', '.join(STATION_IDS) if STATION_IDS else '[none]'}\")\nif invalid_requested_ids:\n    print(f\"  Ignored non-catalog station IDs: {', '.join(invalid_requested_ids)}\")\nif excluded_ids:\n    print(f\"  Excluded station IDs applied: {', '.join(excluded_ids)}\")\n\nif not STATION_IDS:\n    raise RuntimeError(\"No active station IDs available after catalog resolution and exclusions.\")\n\nstation_rows = []\nfor sid in STATION_IDS:\n    try:\n        meta = get_station_metadata(sid)\n    except Exception as exc:\n        logger.warning(\"Skipping station %s due to metadata error: %s\", sid, exc)\n        continue\n\n    name = meta.get(\"name\", sid)\n    shefcode = meta.get(\"shefcode\")\n\n    lat = pd.to_numeric(meta.get(\"lat\"), errors=\"coerce\")\n    lon = pd.to_numeric(meta.get(\"lng\"), errors=\"coerce\")\n\n    if not valid_lat_lon(lat, lon):\n        fallback = STATION_COORDS_FALLBACK.get(sid)\n        if fallback:\n            lat, lon = fallback\n            logger.warning(\"Using fallback coordinates for station %s\", sid)\n\n    if not valid_lat_lon(lat, lon):\n        logger.warning(\"Skipping station %s due to invalid coordinates and no fallback.\", sid)\n        continue\n\n    flood = meta.get(\"floodlevels\") or {}\n    minor = flood.get(\"nos_minor\") or flood.get(\"action\")\n    moderate = flood.get(\"nos_moderate\")\n    major = flood.get(\"nos_major\")\n\n    station_rows.append(\n        {\n            \"station_id\": sid,\n            \"station_name\": name,\n            \"shefcode\": shefcode,\n            \"lat\": float(lat),\n            \"lon\": float(lon),\n            \"minor\": pd.to_numeric(minor, errors=\"coerce\"),\n            \"moderate\": pd.to_numeric(moderate, errors=\"coerce\"),\n            \"major\": pd.to_numeric(major, errors=\"coerce\"),\n        }\n    )\n\nif not station_rows:\n    raise RuntimeError(\"No stations with valid metadata/coordinates were available after resolution.\")\n\nstation_meta_df = pd.DataFrame(station_rows)\ndisplay(station_meta_df)\n\nobs_frames = []\nfor rec in station_rows:\n    sid = rec[\"station_id\"]\n    try:\n        obs_df = fetch_station_observations(sid, BEGIN_DATE, END_DATE)\n    except Exception as exc:\n        msg = str(exc)\n        if \"There is no\" in msg and \"for the station\" in msg:\n            logger.info(\"Skipping station %s due to datum incompatibility: %s\", sid, exc)\n        else:\n            logger.warning(\"Skipping station %s due to observation fetch error: %s\", sid, exc)\n        continue\n\n    if obs_df.empty:\n        logger.info(\"No observations returned for station %s\", sid)\n        continue\n\n    obs_df[\"time_utc\"] = pd.to_datetime(obs_df[\"t\"], utc=True, errors=\"coerce\")\n    obs_df = obs_df.dropna(subset=[\"time_utc\"]).copy()\n\n    if \"f\" in obs_df.columns:\n        obs_df[\"flags\"] = obs_df[\"f\"].astype(str)\n    else:\n        obs_df[\"flags\"] = \"\"\n\n    obs_df[\"f\"] = obs_df[\"flags\"].apply(split_primary_flag)\n\n    obs_df[\"station_id\"] = sid\n    obs_df[\"station_name\"] = rec[\"station_name\"]\n    obs_df[\"shefcode\"] = rec[\"shefcode\"]\n    obs_df[\"lat\"] = rec[\"lat\"]\n    obs_df[\"lon\"] = rec[\"lon\"]\n    obs_df[\"minor\"] = rec[\"minor\"]\n    obs_df[\"moderate\"] = rec[\"moderate\"]\n    obs_df[\"major\"] = rec[\"major\"]\n    obs_df[\"datum\"] = NOAA_DATUM\n    obs_df[\"units\"] = NOAA_UNITS\n    obs_df[\"run_utc\"] = RUN_UTC.isoformat()\n\n    if \"q\" not in obs_df.columns:\n        obs_df[\"q\"] = None\n\n    keep_cols = [\n        \"station_id\",\n        \"station_name\",\n        \"shefcode\",\n        \"time_utc\",\n        \"t\",\n        \"v\",\n        \"s\",\n        \"f\",\n        \"flags\",\n        \"q\",\n        \"lat\",\n        \"lon\",\n        \"minor\",\n        \"moderate\",\n        \"major\",\n        \"datum\",\n        \"units\",\n        \"run_utc\",\n    ]\n    obs_frames.append(obs_df[keep_cols])\n\nif not obs_frames:\n    raise RuntimeError(\"No NOAA observations were returned for any active station.\")\n\nwater_df = pd.concat(obs_frames, ignore_index=True)\nwater_df = water_df.sort_values([\"station_id\", \"time_utc\"]).reset_index(drop=True)\nwater_df = water_df[water_df.apply(lambda r: valid_lat_lon(r[\"lat\"], r[\"lon\"]), axis=1)].copy()\n\nlatest_df = (\n    water_df.sort_values(\"time_utc\")\n    .groupby(\"station_id\", as_index=False)\n    .tail(1)\n    .reset_index(drop=True)\n)\n\n# Station-level analytics for one comprehensive HTML report\nstation_agg = water_df.groupby(\"station_id\", as_index=False).agg(\n    obs_count=(\"v\", \"count\"),\n    peak_value=(\"v\", \"max\"),\n    mean_value=(\"v\", \"mean\"),\n)\n\nstation_summary_df = (\n    latest_df[[\"station_id\", \"station_name\", \"time_utc\", \"v\", \"q\", \"lat\", \"lon\", \"minor\", \"moderate\", \"major\"]]\n    .rename(columns={\"time_utc\": \"latest_time_utc\", \"v\": \"latest_value\", \"q\": \"latest_quality\"})\n    .merge(station_agg, on=\"station_id\", how=\"left\")\n    .sort_values([\"latest_value\", \"station_id\"], ascending=[False, True])\n    .reset_index(drop=True)\n)\n\nprint(f\"Total observations prepared: {len(water_df)}\")\nprint(f\"Active stations with observations: {station_summary_df['station_id'].nunique()}\")\ndisplay(station_summary_df)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Build one comprehensive standalone HTML report\n# =================================================================================\ntrace_station_ids = station_summary_df[\"station_id\"].astype(str).tolist()\nstation_name_lookup = {\n    row[\"station_id\"]: row[\"station_name\"] for _, row in station_summary_df.iterrows()\n}\n\nfig = make_subplots(\n    rows=3,\n    cols=1,\n    specs=[[{\"type\": \"xy\"}], [{\"type\": \"xy\"}], [{\"type\": \"table\"}]],\n    row_heights=[0.58, 0.22, 0.20],\n    vertical_spacing=0.08,\n    subplot_titles=(\n        \"Observed Water-Level Time Series by Station\",\n        \"Latest Water Level by Station\",\n        \"Station Summary\",\n    ),\n)\n\n# Row 1: one line per station\nfor sid in trace_station_ids:\n    sdf = water_df[water_df[\"station_id\"] == sid].copy()\n    sname = station_name_lookup.get(sid, sid)\n    fig.add_trace(\n        go.Scatter(\n            x=sdf[\"time_utc\"],\n            y=sdf[\"v\"],\n            mode=\"lines\",\n            name=f\"{sid} | {sname}\",\n            hovertemplate=(\n                \"Station: \" + sid + \"<br>\"\n                + \"Name: \" + sname + \"<br>\"\n                + \"Time (UTC): %{x}<br>\"\n                + f\"Water level ({NOAA_UNITS}): %{{y:.3f}}<extra></extra>\"\n            ),\n        ),\n        row=1,\n        col=1,\n    )\n\n# Row 2: latest value bar chart\nfig.add_trace(\n    go.Bar(\n        x=station_summary_df[\"station_id\"],\n        y=station_summary_df[\"latest_value\"],\n        text=station_summary_df[\"latest_value\"].map(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"nan\"),\n        textposition=\"outside\",\n        name=\"Latest level\",\n        marker={\"color\": \"rgba(58, 126, 184, 0.85)\"},\n        hovertemplate=(\n            \"Station: %{x}<br>\"\n            + \"Latest level: %{y:.3f}<br>\"\n            + \"Quality: %{customdata[0]}<br>\"\n            + \"Latest time (UTC): %{customdata[1]}<extra></extra>\"\n        ),\n        customdata=station_summary_df[[\"latest_quality\", \"latest_time_utc\"]].astype(str).values,\n        showlegend=False,\n    ),\n    row=2,\n    col=1,\n)\n\n# Row 3: summary table\nfig.add_trace(\n    go.Table(\n        header={\n            \"values\": [\n                \"Station ID\",\n                \"Station Name\",\n                \"Latest UTC\",\n                f\"Latest ({NOAA_UNITS})\",\n                f\"Peak ({NOAA_UNITS})\",\n                \"Obs Count\",\n                \"Quality\",\n            ],\n            \"fill_color\": \"#1f3a56\",\n            \"font\": {\"color\": \"white\", \"size\": 12},\n            \"align\": \"left\",\n        },\n        cells={\n            \"values\": [\n                station_summary_df[\"station_id\"],\n                station_summary_df[\"station_name\"],\n                station_summary_df[\"latest_time_utc\"].astype(str),\n                station_summary_df[\"latest_value\"].map(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"nan\"),\n                station_summary_df[\"peak_value\"].map(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"nan\"),\n                station_summary_df[\"obs_count\"].astype(int),\n                station_summary_df[\"latest_quality\"].astype(str),\n            ],\n            \"align\": \"left\",\n            \"font\": {\"size\": 11},\n            \"height\": 24,\n        },\n    ),\n    row=3,\n    col=1,\n)\n\n# Optional station filter buttons for row-1 traces\nstation_trace_count = len(trace_station_ids)\nbutton_all_visibility = [True] * station_trace_count + [True, True]\nbuttons = [\n    {\n        \"label\": \"All Stations\",\n        \"method\": \"update\",\n        \"args\": [{\"visible\": button_all_visibility}],\n    }\n]\n\nfor idx, sid in enumerate(trace_station_ids):\n    visibility = [False] * station_trace_count + [True, True]\n    visibility[idx] = True\n    buttons.append(\n        {\n            \"label\": sid,\n            \"method\": \"update\",\n            \"args\": [{\"visible\": visibility}],\n        }\n    )\n\nfig.update_layout(\n    title=(\n        f\"NOAA CO-OPS Puerto Rico Water Levels | \"\n        f\"Stations: {station_trace_count} | \"\n        f\"Run UTC: {RUN_UTC.strftime('%Y-%m-%d %H:%M:%S')}\"\n    ),\n    template=\"plotly_white\",\n    height=1100,\n    hovermode=\"x unified\",\n    updatemenus=[\n        {\n            \"buttons\": buttons,\n            \"direction\": \"down\",\n            \"showactive\": True,\n            \"x\": 1.01,\n            \"xanchor\": \"left\",\n            \"y\": 1.16,\n            \"yanchor\": \"top\",\n        }\n    ],\n    margin={\"l\": 50, \"r\": 220, \"t\": 130, \"b\": 50},\n)\n\nfig.update_xaxes(title_text=\"Time (UTC)\", row=1, col=1)\nfig.update_yaxes(title_text=f\"Water level ({NOAA_UNITS})\", row=1, col=1)\nfig.update_xaxes(title_text=\"Station ID\", row=2, col=1)\nfig.update_yaxes(title_text=f\"Latest water level ({NOAA_UNITS})\", row=2, col=1)\n\n# Exports (single HTML artifact + CSV data files)\nwater_df.to_csv(OUTPUT_CSV, index=False)\nstation_summary_df.to_csv(OUTPUT_STATION_CSV, index=False)\nfig.write_html(OUTPUT_HTML, include_plotlyjs=\"inline\", full_html=True)\n\nprint(\"Export complete:\")\nprint(f\"  HTML report: {OUTPUT_HTML}\")\nprint(f\"  Time-series CSV: {OUTPUT_CSV}\")\nprint(f\"  Station summary CSV: {OUTPUT_STATION_CSV}\")\n\nfig.show()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Notes\n\n- This notebook is API-first and resilient by design.\n- Live station catalogs are preferred over hardwired IDs.\n- Non-retryable NOAA client errors are handled without unnecessary retry storms.\n- This notebook intentionally omits AGOL maintenance/action cells.\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}