{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 01 Ingest Flood Hazard Feeds (NOAA + NWS)\n\nStage: `01_ingest_hazard`\nDiscipline: hazard data generation and normalization.\n\nOutputs written to:\n- `JupyterNotebooks/outputs/index_pipeline/01_ingest/noaa_station_catalog.csv`\n- `JupyterNotebooks/outputs/index_pipeline/01_ingest/noaa_station_timeseries.csv`\n- `JupyterNotebooks/outputs/index_pipeline/01_ingest/flood_station_latest_features.csv`\n- `JupyterNotebooks/outputs/index_pipeline/01_ingest/nws_alerts_enriched.csv`\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Setup\nimport importlib.util\nimport subprocess\nimport sys\nimport logging\nimport os\nimport time\nfrom datetime import datetime, timedelta, timezone\nfrom pathlib import Path\n\n\ndef ensure_packages(packages):\n    missing = [p for p in packages if importlib.util.find_spec(p) is None]\n    if missing:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", *missing])\n\n\nensure_packages([\"pandas\", \"numpy\", \"requests\"])\n\nimport numpy as np\nimport pandas as pd\nimport requests\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\nlogger = logging.getLogger(\"index-pipeline-stage01\")\n\n\ndef find_repo_root():\n    p = Path.cwd().resolve()\n    for c in [p, *p.parents]:\n        if (c / \"JupyterNotebooks\").exists():\n            return c\n    return p\n\n\nREPO_ROOT = find_repo_root()\nOUTPUT_DIR = REPO_ROOT / \"JupyterNotebooks\" / \"outputs\" / \"index_pipeline\" / \"01_ingest\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Repo root: {REPO_ROOT}\")\nprint(f\"Output dir: {OUTPUT_DIR}\")\n\ntry:\n    from IPython.display import display\nexcept ImportError:\n    display = print\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Configuration\nNOAA_STATE = \"PR\"\nNOAA_PRODUCT = os.environ.get(\"NOAA_PRODUCT\", \"water_level\")\nNOAA_DATUM = os.environ.get(\"NOAA_DATUM\", \"MLLW\")\nNOAA_TIME_ZONE = os.environ.get(\"NOAA_TIME_ZONE\", \"gmt\")\nNOAA_UNITS = os.environ.get(\"NOAA_UNITS\", \"metric\")\nLOOKBACK_HOURS = int(os.environ.get(\"LOOKBACK_HOURS\", \"72\"))\n\nRAW_STATION_IDS = os.environ.get(\"NOAA_STATION_IDS\", \"\").strip()\nEXCLUDE_STATION_IDS = os.environ.get(\"NOAA_EXCLUDE_STATION_IDS\", \"\").strip()\nMAX_ACTIVE_STATIONS = int(os.environ.get(\"MAX_ACTIVE_STATIONS\", \"100\"))\nREQUIRE_TIDAL_FOR_DATUM = os.environ.get(\"REQUIRE_TIDAL_FOR_DATUM\", \"true\").lower() in (\"1\", \"true\", \"yes\")\n\nCATALOG_TIMEOUT_SECONDS = int(os.environ.get(\"CATALOG_TIMEOUT_SECONDS\", \"90\"))\nCATALOG_MAX_BYTES = int(os.environ.get(\"CATALOG_MAX_BYTES\", \"5000000\"))\nCATALOG_MAX_ROWS = int(os.environ.get(\"CATALOG_MAX_ROWS\", \"5000\"))\n\nPR_BBOX = {\n    \"min_lon\": -68.5,\n    \"max_lon\": -65.0,\n    \"min_lat\": 17.5,\n    \"max_lat\": 18.9,\n}\n\nMDAPI_BASE = \"https://api.tidesandcurrents.noaa.gov/mdapi/prod/webapi\"\nDATAGETTER_URL = \"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\nNWS_ALERTS_URL = \"https://api.weather.gov/alerts/active\"\nNWS_HEADERS = {\n    \"User-Agent\": \"gmu-daen-index-pipeline/1.0\",\n    \"Accept\": \"application/geo+json\",\n}\n\nRUN_UTC = datetime.now(timezone.utc)\nBEGIN_UTC = RUN_UTC - timedelta(hours=LOOKBACK_HOURS)\nBEGIN_DATE = BEGIN_UTC.strftime(\"%Y%m%d %H:%M\")\nEND_DATE = RUN_UTC.strftime(\"%Y%m%d %H:%M\")\n\nprint(\"Configuration ready\")\nprint(f\"  NOAA_DATUM={NOAA_DATUM} LOOKBACK_HOURS={LOOKBACK_HOURS}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Helper functions\n\ndef api_get_json(url, params=None, timeout=60, retries=3, backoff_seconds=2, max_bytes=None, headers=None):\n    last_err = None\n    for attempt in range(1, retries + 1):\n        try:\n            resp = requests.get(url, params=params, timeout=timeout, headers=headers)\n            if max_bytes is not None and len(resp.content or b\"\") > max_bytes:\n                raise RuntimeError(f\"Payload exceeded max bytes for {url}\")\n\n            if resp.status_code >= 400:\n                msg = \"\"\n                try:\n                    payload = resp.json()\n                    msg = (payload.get(\"error\") or {}).get(\"message\", \"\")\n                except Exception:\n                    msg = (resp.text or \"\")[:300]\n\n                if 400 <= resp.status_code < 500 and resp.status_code not in (408, 429):\n                    raise RuntimeError(f\"Non-retryable HTTP {resp.status_code}: {msg}\")\n\n                resp.raise_for_status()\n\n            return resp.json()\n        except Exception as exc:\n            last_err = exc\n            if \"Non-retryable HTTP\" in str(exc):\n                break\n            if attempt < retries:\n                wait_s = backoff_seconds ** attempt\n                logger.warning(\"Request failed (%s/%s). Retrying in %ss. %s\", attempt, retries, wait_s, exc)\n                time.sleep(wait_s)\n    raise RuntimeError(f\"Request failed after {retries} attempts: {last_err}\")\n\n\ndef parse_station_csv(value):\n    return [sid.strip() for sid in str(value).split(\",\") if sid and sid.strip()]\n\n\ndef get_pr_station_catalog(state=\"PR\"):\n    payload = api_get_json(\n        f\"{MDAPI_BASE}/stations.json\",\n        params={\"state\": state},\n        timeout=CATALOG_TIMEOUT_SECONDS,\n        max_bytes=CATALOG_MAX_BYTES,\n    )\n    rows = payload.get(\"stations\", [])\n    if not rows:\n        raise RuntimeError(\"No station rows returned from NOAA catalog\")\n    if len(rows) > CATALOG_MAX_ROWS:\n        raise RuntimeError(f\"Catalog row count {len(rows)} exceeds configured limit {CATALOG_MAX_ROWS}\")\n\n    df = pd.DataFrame(rows)\n    df[\"id\"] = df[\"id\"].astype(str)\n    for c in [\"lat\", \"lng\"]:\n        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n\n    df = df[\n        (df[\"lng\"] >= PR_BBOX[\"min_lon\"])\n        & (df[\"lng\"] <= PR_BBOX[\"max_lon\"])\n        & (df[\"lat\"] >= PR_BBOX[\"min_lat\"])\n        & (df[\"lat\"] <= PR_BBOX[\"max_lat\"])\n    ].copy()\n\n    keep = [c for c in [\"id\", \"name\", \"state\", \"lat\", \"lng\", \"shefcode\", \"tidal\"] if c in df.columns]\n    return df[keep].drop_duplicates(subset=[\"id\"]).sort_values(\"id\").reset_index(drop=True)\n\n\ndef resolve_station_ids(catalog_df):\n    catalog_ids = sorted(catalog_df[\"id\"].astype(str).tolist())\n    catalog_set = set(catalog_ids)\n    requested = parse_station_csv(RAW_STATION_IDS)\n    excluded = set(parse_station_csv(EXCLUDE_STATION_IDS))\n    invalid = [sid for sid in requested if sid not in catalog_set]\n\n    if requested:\n        selected = [sid for sid in requested if sid in catalog_set]\n        mode = \"manual intersect live catalog\"\n        if not selected:\n            selected = catalog_ids\n            mode = \"auto fallback (full live catalog)\"\n    else:\n        selected = catalog_ids\n        mode = \"auto (full live catalog)\"\n\n    if excluded:\n        selected = [sid for sid in selected if sid not in excluded]\n\n    if len(selected) > MAX_ACTIVE_STATIONS:\n        logger.warning(\"Truncating station list %s -> %s due to MAX_ACTIVE_STATIONS\", len(selected), MAX_ACTIVE_STATIONS)\n        selected = selected[:MAX_ACTIVE_STATIONS]\n\n    return selected, invalid, sorted(excluded), mode\n\n\ndef fetch_station_metadata(station_id):\n    payload = api_get_json(\n        f\"{MDAPI_BASE}/stations/{station_id}.json\",\n        params={\"expand\": \"floodlevels,details,sensors\"},\n        timeout=90,\n        max_bytes=CATALOG_MAX_BYTES,\n    )\n    stations = payload.get(\"stations\", [])\n    if not stations:\n        raise RuntimeError(f\"No metadata for station {station_id}\")\n    return stations[0]\n\n\ndef fetch_station_series(station_id):\n    payload = api_get_json(\n        DATAGETTER_URL,\n        params={\n            \"product\": NOAA_PRODUCT,\n            \"application\": \"GMU_INDEX_PIPELINE\",\n            \"begin_date\": BEGIN_DATE,\n            \"end_date\": END_DATE,\n            \"datum\": NOAA_DATUM,\n            \"station\": station_id,\n            \"time_zone\": NOAA_TIME_ZONE,\n            \"units\": NOAA_UNITS,\n            \"format\": \"json\",\n        },\n        timeout=120,\n        max_bytes=CATALOG_MAX_BYTES,\n    )\n\n    if \"error\" in payload:\n        msg = (payload[\"error\"] or {}).get(\"message\", \"\")\n        if \"No data was found\" in msg:\n            return pd.DataFrame(columns=[\"t\", \"v\", \"s\", \"f\", \"q\"])\n        raise RuntimeError(msg)\n\n    rows = payload.get(\"data\", [])\n    if not rows:\n        return pd.DataFrame(columns=[\"t\", \"v\", \"s\", \"f\", \"q\"])\n\n    df = pd.DataFrame(rows)\n    for c in [\"v\", \"s\"]:\n        if c in df.columns:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\n\ndef map_alert_score(event, severity):\n    event_text = (event or \"\").lower()\n    severity_text = (severity or \"\").lower()\n\n    if \"flash flood warning\" in event_text:\n        return 100\n    if \"flood warning\" in event_text:\n        return 70\n    if \"flood watch\" in event_text:\n        return 40\n\n    sev_map = {\n        \"extreme\": 100,\n        \"severe\": 80,\n        \"moderate\": 60,\n        \"minor\": 40,\n        \"unknown\": 25,\n    }\n    return sev_map.get(severity_text, 25)\n\n\ndef normalize_linear(value, low, high):\n    if pd.isna(value):\n        return np.nan\n    if high <= low:\n        return np.nan\n    return float(np.clip((value - low) / (high - low), 0, 1) * 100)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Ingest NOAA + NWS and build flood hazard features\ncatalog_pull_utc = datetime.now(timezone.utc)\nstation_catalog_df = get_pr_station_catalog(NOAA_STATE)\n\nif REQUIRE_TIDAL_FOR_DATUM and NOAA_DATUM.upper() != \"STND\" and \"tidal\" in station_catalog_df.columns:\n    pre = len(station_catalog_df)\n    station_catalog_df[\"tidal\"] = station_catalog_df[\"tidal\"].fillna(False).astype(bool)\n    station_catalog_df = station_catalog_df[station_catalog_df[\"tidal\"]].copy()\n    logger.info(\"Datum compatibility filter: %s -> %s stations\", pre, len(station_catalog_df))\n\nstation_ids, invalid_requested, excluded_applied, selection_mode = resolve_station_ids(station_catalog_df)\nprint(f\"Selection mode: {selection_mode}\")\nprint(f\"Selected stations: {len(station_ids)}\")\nif invalid_requested:\n    print(f\"Ignored station IDs: {invalid_requested}\")\nif excluded_applied:\n    print(f\"Excluded station IDs: {excluded_applied}\")\n\nstation_rows = []\ntimeseries_frames = []\n\nfor sid in station_ids:\n    try:\n        meta = fetch_station_metadata(sid)\n        ts = fetch_station_series(sid)\n    except Exception as exc:\n        logger.warning(\"Skipping station %s: %s\", sid, exc)\n        continue\n\n    if ts.empty:\n        logger.info(\"No timeseries rows for station %s\", sid)\n        continue\n\n    ts[\"time_utc\"] = pd.to_datetime(ts[\"t\"], utc=True, errors=\"coerce\")\n    ts = ts.dropna(subset=[\"time_utc\"]).sort_values(\"time_utc\").copy()\n\n    lat = pd.to_numeric(meta.get(\"lat\"), errors=\"coerce\")\n    lon = pd.to_numeric(meta.get(\"lng\"), errors=\"coerce\")\n    if pd.isna(lat) or pd.isna(lon):\n        continue\n\n    flood = meta.get(\"floodlevels\") or {}\n    minor = pd.to_numeric(flood.get(\"nos_minor\") or flood.get(\"action\"), errors=\"coerce\")\n    moderate = pd.to_numeric(flood.get(\"nos_moderate\"), errors=\"coerce\")\n    major = pd.to_numeric(flood.get(\"nos_major\"), errors=\"coerce\")\n\n    latest = ts.iloc[-1]\n    prev = ts.iloc[-2] if len(ts) > 1 else latest\n    latest_value = pd.to_numeric(latest.get(\"v\"), errors=\"coerce\")\n    prev_value = pd.to_numeric(prev.get(\"v\"), errors=\"coerce\")\n\n    delta_hours = max((latest[\"time_utc\"] - prev[\"time_utc\"]).total_seconds() / 3600.0, 1e-6)\n    rise_rate = (latest_value - prev_value) / delta_hours if pd.notna(latest_value) and pd.notna(prev_value) else np.nan\n\n    exceed_ratio = np.nan\n    if pd.notna(minor) and pd.notna(major) and major > minor and pd.notna(latest_value):\n        exceed_ratio = (latest_value - minor) / (major - minor)\n\n    exceed_score = normalize_linear(exceed_ratio, 0.0, 1.0)\n    rise_score = normalize_linear(rise_rate, 0.0, 0.30)\n    sensor_hazard_score = np.nanmean([exceed_score, rise_score]) if (pd.notna(exceed_score) or pd.notna(rise_score)) else 0.0\n\n    ts = ts.rename(columns={\"v\": \"water_level\", \"s\": \"sigma\", \"q\": \"quality\", \"f\": \"flags_raw\"})\n    ts[\"flags\"] = ts.get(\"flags_raw\", \"\").astype(str)\n    ts[\"station_id\"] = sid\n    ts[\"station_name\"] = meta.get(\"name\", sid)\n    ts[\"lat\"] = float(lat)\n    ts[\"lon\"] = float(lon)\n    ts[\"minor\"] = minor\n    ts[\"moderate\"] = moderate\n    ts[\"major\"] = major\n    ts[\"datum\"] = NOAA_DATUM\n    ts[\"units\"] = NOAA_UNITS\n    ts[\"run_utc\"] = RUN_UTC.isoformat()\n\n    keep_cols = [\n        \"station_id\", \"station_name\", \"time_utc\", \"water_level\", \"sigma\", \"flags\", \"quality\",\n        \"lat\", \"lon\", \"minor\", \"moderate\", \"major\", \"datum\", \"units\", \"run_utc\"\n    ]\n    timeseries_frames.append(ts[keep_cols])\n\n    station_rows.append({\n        \"station_id\": sid,\n        \"station_name\": meta.get(\"name\", sid),\n        \"shefcode\": meta.get(\"shefcode\"),\n        \"lat\": float(lat),\n        \"lon\": float(lon),\n        \"latest_time_utc\": latest[\"time_utc\"],\n        \"latest_water_level\": latest_value,\n        \"rise_rate_per_hour\": rise_rate,\n        \"minor\": minor,\n        \"moderate\": moderate,\n        \"major\": major,\n        \"exceed_ratio\": exceed_ratio,\n        \"exceed_score\": exceed_score,\n        \"rise_score\": rise_score,\n        \"sensor_hazard_score\": float(sensor_hazard_score),\n    })\n\nif not station_rows:\n    raise RuntimeError(\"No station hazard rows were generated. Check NOAA API settings.\")\n\nstation_latest_df = pd.DataFrame(station_rows)\nnoaa_timeseries_df = pd.concat(timeseries_frames, ignore_index=True)\n\n# NWS alerts ingest\nalerts_payload = api_get_json(\n    NWS_ALERTS_URL,\n    params={\"area\": \"PR\"},\n    timeout=60,\n    retries=2,\n    max_bytes=CATALOG_MAX_BYTES,\n    headers=NWS_HEADERS,\n)\n\nalert_rows = []\nfor feat in alerts_payload.get(\"features\", []):\n    props = feat.get(\"properties\", {})\n    event = props.get(\"event\")\n    severity = props.get(\"severity\")\n    score = map_alert_score(event, severity)\n    geocodes = props.get(\"geocode\") or {}\n    ugc = geocodes.get(\"UGC\") or []\n    area_desc = props.get(\"areaDesc\")\n\n    alert_rows.append({\n        \"id\": props.get(\"id\"),\n        \"event\": event,\n        \"severity\": severity,\n        \"urgency\": props.get(\"urgency\"),\n        \"certainty\": props.get(\"certainty\"),\n        \"headline\": props.get(\"headline\"),\n        \"sent\": props.get(\"sent\"),\n        \"onset\": props.get(\"onset\"),\n        \"ends\": props.get(\"ends\"),\n        \"status\": props.get(\"status\"),\n        \"area_desc\": area_desc,\n        \"ugc_list\": \",\".join(ugc) if isinstance(ugc, list) else str(ugc),\n        \"alert_score\": score,\n    })\n\nnws_alerts_df = pd.DataFrame(alert_rows)\nglobal_alert_score = float(nws_alerts_df[\"alert_score\"].max()) if not nws_alerts_df.empty else 0.0\n\nstation_latest_df[\"nws_global_alert_score\"] = global_alert_score\nstation_latest_df[\"flood_hazard_final\"] = station_latest_df[[\"sensor_hazard_score\"]].max(axis=1)\nstation_latest_df[\"flood_hazard_final\"] = station_latest_df[\"flood_hazard_final\"].clip(0, 100)\nstation_latest_df[\"flood_hazard_final\"] = station_latest_df[[\"flood_hazard_final\"]].assign(g=global_alert_score).max(axis=1)\nstation_latest_df[\"catalog_pull_utc\"] = catalog_pull_utc.isoformat()\n\n# Save outputs\nstation_catalog_out = OUTPUT_DIR / \"noaa_station_catalog.csv\"\nnoaa_series_out = OUTPUT_DIR / \"noaa_station_timeseries.csv\"\nstation_latest_out = OUTPUT_DIR / \"flood_station_latest_features.csv\"\nnws_alerts_out = OUTPUT_DIR / \"nws_alerts_enriched.csv\"\n\nstation_catalog_df.to_csv(station_catalog_out, index=False)\nnoaa_timeseries_df.to_csv(noaa_series_out, index=False)\nstation_latest_df.to_csv(station_latest_out, index=False)\nnws_alerts_df.to_csv(nws_alerts_out, index=False)\n\nprint(\"Stage 01 outputs:\")\nprint(f\"  {station_catalog_out}\")\nprint(f\"  {noaa_series_out}\")\nprint(f\"  {station_latest_out}\")\nprint(f\"  {nws_alerts_out}\")\n\nprint(\"\\nPreview flood station features:\")\ndisplay(station_latest_df.head(10))\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}